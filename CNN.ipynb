{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code shows how images, specifically vehicle images like cars, trucks, vans, bikes, motorcycles and others, can be classified using Convolutional Neural Network (CNN). This code is built based on an image classifier developed for the \"Applied Data Science: Machine Learning\" Program from the EPFL Lausanne.\n",
    "The dataset consists of Swissroads data set which contains several hundreds images of vehicles found in the EPFL - Lausanne area including cars, trucks, vans, bikes, motorcycles and others. The dataset is quite small, hence the results, but could easily be extended to include other datasets and other vehicles classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# keras imports\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import backend as K \n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 500\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "NUM_CLASSES = 6\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH)\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.getcwd()  # Get the current working directory\n",
    "data_dir = os.path.join(base_dir, 'swissroads')\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "valid_dir = os.path.join(data_dir, 'valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Utility functions**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'], color = colors[n], label = 'Train ' + label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'], color = colors[n], label = 'Val ' + label, linestyle = \"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "  \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history, label, n):\n",
    "  # Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['accuracy'], color = colors[n], label = 'Train ' + label)\n",
    "    plt.semilogy(history.epoch,  history.history['val_accuracy'], color = colors[n], label = 'Val ' + label, linestyle = \"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "  \n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    metrics = ['loss', 'accuracy']\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\",\" \").capitalize()\n",
    "        plt.subplot(2, 2, n+1)\n",
    "        plt.plot(history.epoch,  history.history[metric], color = colors[0], label = 'Train')\n",
    "        plt.plot(history.epoch, history.history['val_' + metric], color = colors[1], linestyle = \"--\", label = 'Val')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cspace_transf(image):\n",
    "    image = np.array(image)\n",
    "    return cv2.cvtColor(image,cv2.COLOR_RGB2XYZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Using image data augmentation**\n",
    "***\n",
    "\n",
    "\n",
    "Overfitting is more likely to occur with a small number of samples and data augmentation seeks to improve this by providing additional data, based on the originals, with random yet realistic transformations to the training images, such as random horizontal flipping or small random rotations. This helps expose the model to different aspects of the training data while slowing down overfitting. The goal is the model will never see the exact same picture twice during training. This helps expose the model to more aspects of the data and generalize better.\n",
    "\n",
    "The ImageDataGenerator transforms and rescale the images before putting them into a revised dataset. Rescaling gives all pixel channes a value from 0 to 255.\n",
    "\n",
    "Here, I applied rescale, 45 degree rotation, width shift, height shift, horizontal flip and zoom augmentation to the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data augmentation to improve overfitting\n",
    "train_image_generator = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    rotation_range=45,\n",
    "                    width_shift_range=.15,\n",
    "                    height_shift_range=.15,\n",
    "                    horizontal_flip=True,\n",
    "                    brightness_range=[0.2,1.0],\n",
    "                    zoom_range=0.5,\n",
    "                    channel_shift_range=150.0,\n",
    "                    fill_mode='nearest'\n",
    ") # Generator for our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply data augmentation to improve overfitting\n",
    "train_image_generator_cspace = ImageDataGenerator(\n",
    "                    rescale=1./255,\n",
    "                    preprocessing_function = cspace_transf,\n",
    ") # Generator for our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Standardizing the data**\n",
    "***\n",
    "Our image are already in a standard size (256x256), as they are being yielded as contiguous float32 batches by our dataset. However, their RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general we should seek to make your input values small. Here, we will standardize values to be in the [0, 1] by using a Rescaling layer at the start of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only rescale, as we want our model to predict on real world images\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our validation data\n",
    "test_image_generator = ImageDataGenerator(rescale=1./255) # Generator for our test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Data pre-processing**\n",
    "***\n",
    "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class. This class allows to:\n",
    "\n",
    "- configure random transformations and normalization operations to be done on your image data during training\n",
    "- instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs, fit_generator, evaluate_generator and predict_generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_image_generator.flow_from_directory(batch_size = BATCH_SIZE,\n",
    "                                                           directory = train_dir,\n",
    "                                                           shuffle = True,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 280 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen_cspace = train_image_generator_cspace.flow_from_directory(batch_size = BATCH_SIZE,\n",
    "                                                           directory = train_dir,\n",
    "                                                           shuffle = True,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_data_gen = validation_image_generator.flow_from_directory(batch_size = BATCH_SIZE,\n",
    "                                                           directory = valid_dir,\n",
    "                                                           shuffle = True,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = test_image_generator.flow_from_directory(batch_size = BATCH_SIZE,\n",
    "                                                           directory = test_dir,\n",
    "                                                           shuffle = True,\n",
    "                                                           target_size = IMG_SIZE,\n",
    "                                                           class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to check the data format i.e if the RGB channel is coming first or last so that whatever it may be, the model will first check and then get the appropriate input shape be fed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, IMG_WIDTH, IMG_HEIGHT) \n",
    "else: \n",
    "    input_shape = (IMG_WIDTH, IMG_HEIGHT, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Build the model**\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right tool for an image classification job is a convnet, so let's try to train one on our data, as an initial baseline. Since we only have few examples, our number one concern should be overfitting. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three, images of people who are sailors, and among them only one lumberjack wears a cap, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
    "\n",
    "Data augmentation is one way to fight overfitting, but it isn't enough since our augmented samples are still highly correlated. Your main focus for fighting overfitting should be the entropic capacity of your model --how much information your model is allowed to store. A model that can store a lot of information has the potential to be more accurate by leveraging more features, but it is also more at risk to start storing irrelevant features. Meanwhile, a model that can only store a few features will have to focus on the most significant features found in the data, and these are more likely to be truly relevant and to generalize better.\n",
    "\n",
    "There are different ways to modulate entropic capacity. The main one is the choice of the number of parameters in the model, i.e. the number of layers and the size of each layer. Another way is the use of weight regularization, such as L1 or L2 regularization, which consists in forcing model weights to taker smaller values.\n",
    "\n",
    "\n",
    "The architectural decisions we will be making are the number of layers, the number of filters, and the size of the filters. In our case we will use a rather small convnet with few layers and few filters per layer, alongside data augmentation and dropout. Dropout also helps reduce overfitting, by preventing a layer from seeing twice the exact same pattern, thus acting in a way analoguous to data augmentation (you could say that both dropout and data augmentation tend to disrupt random correlations occuring in the data). \n",
    "\n",
    "This is very similar to the architectures that Yann LeCun advocated in the 1990s for image classification (with the exception of ReLU).\n",
    "\n",
    "- **Conv2D** is the layer to convolve the image into multiple images\n",
    "- **Activation** is the activation function.\n",
    "- **MaxPooling2D** is used to max pool the value from the given size matrix and same is used for the next 2 layers. then, Flatten is used to flatten the dimensions of the image obtained after convolving it.\n",
    "- **Dense** is used to make this a fully connected model and is the hidden layer.\n",
    "- **Dropout** is used to avoid overfitting on the dataset.\n",
    "- **Dense** is the output layer contains only one neuron which decide to which category image belongs.\n",
    "\n",
    "Typically the number of layers starts small and grows as the complexity realized by the convoluted layers grows. The number of filters in a layer should be set at ratios of 32, 64, 128, 256, 512 and so on according to one source. In this case I have elected to make four convolutional layers with succeeding filter sizes of 16, 32, 64, 128.\n",
    "\n",
    "Filters have odd values since they need to be centered on the pixel being convolved. A 3x3 filter is usual although larger ones of 5x5 up to 7x7 may work better on larger images. After multiple trials and experiements I decided to go a considerably bigger filter of 11x11. The reason for this is that for this particular image set, a big number of pixels is probably necessary for the network recognize the object (the objects occupy a very part of the image). Max pool layers typically have a pool size of (2, 2) and are applied after each convolutional layer.\n",
    "\n",
    "After flattening I chose to set dropout to 0.1 and 0.5 respectively. \n",
    "\n",
    "The model was given a single dense layer with 128 neurons with activation set to 'relu' as usual. The number of output elements is always set to the number of classes and activation to 'softmax'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "        \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size=(11, 11), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(32, (11, 11), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(64, (11, 11), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(128, (11, 11), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(lr = 1e-5), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Model Architecture**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Early stopping**\n",
    "\n",
    "When we’re training a learning algorithm iteratively, we can measure how well each iteration of the model performs. Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data. Early stopping refers stopping the training process before the learner passes that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_accuracy', \n",
    "    verbose = 2,\n",
    "    patience = 30,\n",
    "    mode = 'max',\n",
    "    restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_acc = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_accuracy', \n",
    "    factor=0.1, \n",
    "    patience=50, \n",
    "    verbose=2, \n",
    "#     min_delta=1e-4, \n",
    "    cooldown=1,\n",
    "    mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_save = tf.keras.callbacks.ModelCheckpoint('model_best.h5', \n",
    "                           save_best_only=True, \n",
    "                           monitor='val_accuracy', \n",
    "                           mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_2 = [early_stopping, reduce_lr_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [reduce_lr_acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Train the model**\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_STEPS = int(np.ceil(train_data_gen.n / float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_STEPS = int(np.ceil(valid_data_gen.n / float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STEPS = int(np.ceil(test_data_gen.n / float(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_58 (Conv2D)           (None, 256, 256, 16)      5824      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_58 (Batc (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 128, 128, 32)      61984     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_59 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 64, 64, 64)        247872    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 32, 32, 128)       991360    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 5,503,206\n",
      "Trainable params: 5,502,726\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 2.9614 - accuracy: 0.1841 - val_loss: 1.7805 - val_accuracy: 0.2374\n",
      "Epoch 2/1000\n",
      "18/18 [==============================] - 152s 8s/step - loss: 2.5336 - accuracy: 0.1862 - val_loss: 1.7684 - val_accuracy: 0.2302\n",
      "Epoch 3/1000\n",
      "18/18 [==============================] - 134s 7s/step - loss: 2.5734 - accuracy: 0.2217 - val_loss: 1.7669 - val_accuracy: 0.2158\n",
      "Epoch 4/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 2.4624 - accuracy: 0.1843 - val_loss: 1.7575 - val_accuracy: 0.2590\n",
      "Epoch 5/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 2.2050 - accuracy: 0.2189 - val_loss: 1.7520 - val_accuracy: 0.2374\n",
      "Epoch 6/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.2728 - accuracy: 0.2165 - val_loss: 1.7392 - val_accuracy: 0.2374\n",
      "Epoch 7/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 2.1976 - accuracy: 0.2156 - val_loss: 1.7412 - val_accuracy: 0.2374\n",
      "Epoch 8/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 2.0946 - accuracy: 0.2700 - val_loss: 1.7206 - val_accuracy: 0.3381\n",
      "Epoch 9/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 2.2353 - accuracy: 0.2332 - val_loss: 1.7618 - val_accuracy: 0.2374\n",
      "Epoch 10/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.0584 - accuracy: 0.2864 - val_loss: 1.7465 - val_accuracy: 0.2446\n",
      "Epoch 11/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 2.0764 - accuracy: 0.2430 - val_loss: 1.8039 - val_accuracy: 0.2590\n",
      "Epoch 12/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.9672 - accuracy: 0.2607 - val_loss: 1.7825 - val_accuracy: 0.2806\n",
      "Epoch 13/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.8688 - accuracy: 0.2961 - val_loss: 1.8043 - val_accuracy: 0.2446\n",
      "Epoch 14/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.7918 - accuracy: 0.2602 - val_loss: 1.8074 - val_accuracy: 0.2662\n",
      "Epoch 15/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.9375 - accuracy: 0.2881 - val_loss: 1.9255 - val_accuracy: 0.2878\n",
      "Epoch 16/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.8005 - accuracy: 0.3474 - val_loss: 1.8402 - val_accuracy: 0.2734\n",
      "Epoch 17/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 2.0477 - accuracy: 0.2156 - val_loss: 1.7773 - val_accuracy: 0.2950\n",
      "Epoch 18/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.8652 - accuracy: 0.2949 - val_loss: 1.7172 - val_accuracy: 0.3381\n",
      "Epoch 19/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.9312 - accuracy: 0.3247 - val_loss: 1.7406 - val_accuracy: 0.3237\n",
      "Epoch 20/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.8799 - accuracy: 0.2208 - val_loss: 1.7549 - val_accuracy: 0.3237\n",
      "Epoch 21/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.7922 - accuracy: 0.3086 - val_loss: 1.6772 - val_accuracy: 0.3453\n",
      "Epoch 22/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.8706 - accuracy: 0.2793 - val_loss: 1.6670 - val_accuracy: 0.3813\n",
      "Epoch 23/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.8648 - accuracy: 0.2911 - val_loss: 1.6651 - val_accuracy: 0.3309\n",
      "Epoch 24/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.7321 - accuracy: 0.3313 - val_loss: 1.6260 - val_accuracy: 0.3813\n",
      "Epoch 25/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.8703 - accuracy: 0.2474 - val_loss: 1.6132 - val_accuracy: 0.4029\n",
      "Epoch 26/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.7219 - accuracy: 0.3295 - val_loss: 1.6255 - val_accuracy: 0.3669\n",
      "Epoch 27/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.6825 - accuracy: 0.3587 - val_loss: 1.6400 - val_accuracy: 0.3525\n",
      "Epoch 28/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.6044 - accuracy: 0.3778 - val_loss: 1.5892 - val_accuracy: 0.3525\n",
      "Epoch 29/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.7256 - accuracy: 0.2931 - val_loss: 1.5759 - val_accuracy: 0.3597\n",
      "Epoch 30/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.7914 - accuracy: 0.2627 - val_loss: 1.5971 - val_accuracy: 0.3165\n",
      "Epoch 31/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.7564 - accuracy: 0.2655 - val_loss: 1.6146 - val_accuracy: 0.3381\n",
      "Epoch 32/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.6937 - accuracy: 0.3382 - val_loss: 1.5878 - val_accuracy: 0.3669\n",
      "Epoch 33/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.7888 - accuracy: 0.2700 - val_loss: 1.5884 - val_accuracy: 0.3669\n",
      "Epoch 34/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.8152 - accuracy: 0.3100 - val_loss: 1.5796 - val_accuracy: 0.3453\n",
      "Epoch 35/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.7911 - accuracy: 0.3046 - val_loss: 1.5891 - val_accuracy: 0.3669\n",
      "Epoch 36/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7580 - accuracy: 0.2915 - val_loss: 1.5635 - val_accuracy: 0.3741\n",
      "Epoch 37/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.8453 - accuracy: 0.2850 - val_loss: 1.5464 - val_accuracy: 0.3957\n",
      "Epoch 38/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7998 - accuracy: 0.2878 - val_loss: 1.5333 - val_accuracy: 0.4173\n",
      "Epoch 39/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6376 - accuracy: 0.3473 - val_loss: 1.5194 - val_accuracy: 0.4101\n",
      "Epoch 40/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7317 - accuracy: 0.3113 - val_loss: 1.5109 - val_accuracy: 0.4029\n",
      "Epoch 41/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.7670 - accuracy: 0.3028 - val_loss: 1.5363 - val_accuracy: 0.3525\n",
      "Epoch 42/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7271 - accuracy: 0.3015 - val_loss: 1.5247 - val_accuracy: 0.3597\n",
      "Epoch 43/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.8681 - accuracy: 0.2774 - val_loss: 1.4810 - val_accuracy: 0.3957\n",
      "Epoch 44/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7143 - accuracy: 0.3138 - val_loss: 1.4721 - val_accuracy: 0.4029\n",
      "Epoch 45/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.5973 - accuracy: 0.3189 - val_loss: 1.4623 - val_accuracy: 0.4245\n",
      "Epoch 46/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.5979 - accuracy: 0.3931 - val_loss: 1.4481 - val_accuracy: 0.4460\n",
      "Epoch 47/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.7131 - accuracy: 0.2534 - val_loss: 1.4645 - val_accuracy: 0.4388\n",
      "Epoch 48/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.8320 - accuracy: 0.2837 - val_loss: 1.4819 - val_accuracy: 0.4173\n",
      "Epoch 49/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6948 - accuracy: 0.3823 - val_loss: 1.4928 - val_accuracy: 0.4101\n",
      "Epoch 50/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.7359 - accuracy: 0.3273 - val_loss: 1.4512 - val_accuracy: 0.4388\n",
      "Epoch 51/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.7371 - accuracy: 0.3358 - val_loss: 1.4202 - val_accuracy: 0.4460\n",
      "Epoch 52/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.6417 - accuracy: 0.3333 - val_loss: 1.4411 - val_accuracy: 0.4604\n",
      "Epoch 53/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6565 - accuracy: 0.3058 - val_loss: 1.4645 - val_accuracy: 0.4245\n",
      "Epoch 54/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.5844 - accuracy: 0.3557 - val_loss: 1.4539 - val_accuracy: 0.4029\n",
      "Epoch 55/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6657 - accuracy: 0.3474 - val_loss: 1.4448 - val_accuracy: 0.4173\n",
      "Epoch 56/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.5498 - accuracy: 0.4013 - val_loss: 1.4535 - val_accuracy: 0.3813\n",
      "Epoch 57/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.6703 - accuracy: 0.3186 - val_loss: 1.4459 - val_accuracy: 0.3813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.6048 - accuracy: 0.3704 - val_loss: 1.4195 - val_accuracy: 0.4388\n",
      "Epoch 59/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6722 - accuracy: 0.3379 - val_loss: 1.4973 - val_accuracy: 0.4173\n",
      "Epoch 60/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.6291 - accuracy: 0.3631 - val_loss: 1.4328 - val_accuracy: 0.3813\n",
      "Epoch 61/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6056 - accuracy: 0.3960 - val_loss: 1.4066 - val_accuracy: 0.4245\n",
      "Epoch 62/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6812 - accuracy: 0.3072 - val_loss: 1.4272 - val_accuracy: 0.4317\n",
      "Epoch 63/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.5108 - accuracy: 0.4258 - val_loss: 1.4389 - val_accuracy: 0.4173\n",
      "Epoch 64/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.7189 - accuracy: 0.2789 - val_loss: 1.4540 - val_accuracy: 0.4317\n",
      "Epoch 65/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6272 - accuracy: 0.3706 - val_loss: 1.4145 - val_accuracy: 0.4460\n",
      "Epoch 66/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.6519 - accuracy: 0.3746 - val_loss: 1.4234 - val_accuracy: 0.4173\n",
      "Epoch 67/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6545 - accuracy: 0.3545 - val_loss: 1.4485 - val_accuracy: 0.4101\n",
      "Epoch 68/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.5639 - accuracy: 0.3672 - val_loss: 1.4155 - val_accuracy: 0.4460\n",
      "Epoch 69/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.7099 - accuracy: 0.3119 - val_loss: 1.3934 - val_accuracy: 0.4173\n",
      "Epoch 70/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.5861 - accuracy: 0.3491 - val_loss: 1.4088 - val_accuracy: 0.4388\n",
      "Epoch 71/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.5485 - accuracy: 0.4029 - val_loss: 1.3508 - val_accuracy: 0.4604\n",
      "Epoch 72/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.4761 - accuracy: 0.4375 - val_loss: 1.4004 - val_accuracy: 0.4317\n",
      "Epoch 73/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6334 - accuracy: 0.3711 - val_loss: 1.3783 - val_accuracy: 0.4245\n",
      "Epoch 74/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.5909 - accuracy: 0.3863 - val_loss: 1.3620 - val_accuracy: 0.4388\n",
      "Epoch 75/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.6711 - accuracy: 0.3271 - val_loss: 1.3873 - val_accuracy: 0.4245\n",
      "Epoch 76/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.5944 - accuracy: 0.3833 - val_loss: 1.3687 - val_accuracy: 0.4317\n",
      "Epoch 77/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6169 - accuracy: 0.3730 - val_loss: 1.3736 - val_accuracy: 0.4245\n",
      "Epoch 78/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6057 - accuracy: 0.3720 - val_loss: 1.3652 - val_accuracy: 0.4604\n",
      "Epoch 79/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.6505 - accuracy: 0.3010 - val_loss: 1.3714 - val_accuracy: 0.4820\n",
      "Epoch 80/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.5677 - accuracy: 0.3573 - val_loss: 1.4072 - val_accuracy: 0.4532\n",
      "Epoch 81/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.6307 - accuracy: 0.3492 - val_loss: 1.3863 - val_accuracy: 0.4388\n",
      "Epoch 82/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.6102 - accuracy: 0.3613 - val_loss: 1.3664 - val_accuracy: 0.4748\n",
      "Epoch 83/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.5128 - accuracy: 0.3837 - val_loss: 1.3266 - val_accuracy: 0.5252\n",
      "Epoch 84/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.5175 - accuracy: 0.3951 - val_loss: 1.3230 - val_accuracy: 0.5252\n",
      "Epoch 85/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.5661 - accuracy: 0.3854 - val_loss: 1.3585 - val_accuracy: 0.4820\n",
      "Epoch 86/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.6290 - accuracy: 0.3706 - val_loss: 1.3859 - val_accuracy: 0.4532\n",
      "Epoch 87/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.6096 - accuracy: 0.3198 - val_loss: 1.3153 - val_accuracy: 0.4964\n",
      "Epoch 88/1000\n",
      "18/18 [==============================] - 216s 12s/step - loss: 1.5542 - accuracy: 0.3847 - val_loss: 1.3254 - val_accuracy: 0.4604\n",
      "Epoch 89/1000\n",
      "18/18 [==============================] - 133s 8s/step - loss: 1.7135 - accuracy: 0.3202 - val_loss: 1.3707 - val_accuracy: 0.4460\n",
      "Epoch 90/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5783 - accuracy: 0.3989 - val_loss: 1.3482 - val_accuracy: 0.4604\n",
      "Epoch 91/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.5325 - accuracy: 0.3834 - val_loss: 1.3234 - val_accuracy: 0.4676\n",
      "Epoch 92/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5078 - accuracy: 0.3971 - val_loss: 1.3177 - val_accuracy: 0.4676\n",
      "Epoch 93/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.4254 - accuracy: 0.4295 - val_loss: 1.3134 - val_accuracy: 0.4820\n",
      "Epoch 94/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4971 - accuracy: 0.3738 - val_loss: 1.3333 - val_accuracy: 0.4676\n",
      "Epoch 95/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.4786 - accuracy: 0.3650 - val_loss: 1.3315 - val_accuracy: 0.4748\n",
      "Epoch 96/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.4525 - accuracy: 0.4786 - val_loss: 1.3120 - val_accuracy: 0.4964\n",
      "Epoch 97/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.4865 - accuracy: 0.4169 - val_loss: 1.3097 - val_accuracy: 0.4748\n",
      "Epoch 98/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.6235 - accuracy: 0.3483 - val_loss: 1.3609 - val_accuracy: 0.4892\n",
      "Epoch 99/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5410 - accuracy: 0.4145 - val_loss: 1.3026 - val_accuracy: 0.5036\n",
      "Epoch 100/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5603 - accuracy: 0.3564 - val_loss: 1.2972 - val_accuracy: 0.4820\n",
      "Epoch 101/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5224 - accuracy: 0.3823 - val_loss: 1.3322 - val_accuracy: 0.4820\n",
      "Epoch 102/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.5732 - accuracy: 0.3363 - val_loss: 1.3365 - val_accuracy: 0.4820\n",
      "Epoch 103/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.5067 - accuracy: 0.4075 - val_loss: 1.3229 - val_accuracy: 0.5036\n",
      "Epoch 104/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5281 - accuracy: 0.3914 - val_loss: 1.2652 - val_accuracy: 0.4892\n",
      "Epoch 105/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.5809 - accuracy: 0.3546 - val_loss: 1.2675 - val_accuracy: 0.5252\n",
      "Epoch 106/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.4077 - accuracy: 0.4217 - val_loss: 1.2705 - val_accuracy: 0.5324\n",
      "Epoch 107/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.5631 - accuracy: 0.3975 - val_loss: 1.2702 - val_accuracy: 0.4964\n",
      "Epoch 108/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.4977 - accuracy: 0.3823 - val_loss: 1.2908 - val_accuracy: 0.5180\n",
      "Epoch 109/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5060 - accuracy: 0.3805 - val_loss: 1.2718 - val_accuracy: 0.5108\n",
      "Epoch 110/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.5274 - accuracy: 0.4029 - val_loss: 1.2565 - val_accuracy: 0.5108\n",
      "Epoch 111/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.4188 - accuracy: 0.4600 - val_loss: 1.2486 - val_accuracy: 0.4748\n",
      "Epoch 112/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5205 - accuracy: 0.4113 - val_loss: 1.2311 - val_accuracy: 0.5108\n",
      "Epoch 113/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.5379 - accuracy: 0.4226 - val_loss: 1.2496 - val_accuracy: 0.5252\n",
      "Epoch 114/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.5004 - accuracy: 0.4429 - val_loss: 1.2964 - val_accuracy: 0.4748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4945 - accuracy: 0.4067 - val_loss: 1.2634 - val_accuracy: 0.5180\n",
      "Epoch 116/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.5338 - accuracy: 0.4126 - val_loss: 1.3326 - val_accuracy: 0.4892\n",
      "Epoch 117/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.5183 - accuracy: 0.3895 - val_loss: 1.2788 - val_accuracy: 0.5468\n",
      "Epoch 118/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.5143 - accuracy: 0.4067 - val_loss: 1.2865 - val_accuracy: 0.5180\n",
      "Epoch 119/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.4738 - accuracy: 0.4319 - val_loss: 1.2940 - val_accuracy: 0.4748\n",
      "Epoch 120/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4627 - accuracy: 0.4603 - val_loss: 1.2716 - val_accuracy: 0.5612\n",
      "Epoch 121/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5194 - accuracy: 0.3871 - val_loss: 1.2708 - val_accuracy: 0.5180\n",
      "Epoch 122/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.4737 - accuracy: 0.3601 - val_loss: 1.2496 - val_accuracy: 0.5180\n",
      "Epoch 123/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5212 - accuracy: 0.3721 - val_loss: 1.2487 - val_accuracy: 0.4892\n",
      "Epoch 124/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4568 - accuracy: 0.4397 - val_loss: 1.2562 - val_accuracy: 0.5036\n",
      "Epoch 125/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.4491 - accuracy: 0.4610 - val_loss: 1.2713 - val_accuracy: 0.4964\n",
      "Epoch 126/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4825 - accuracy: 0.4408 - val_loss: 1.3047 - val_accuracy: 0.5180\n",
      "Epoch 127/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4463 - accuracy: 0.4456 - val_loss: 1.2503 - val_accuracy: 0.5324\n",
      "Epoch 128/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.4610 - accuracy: 0.4055 - val_loss: 1.2514 - val_accuracy: 0.5036\n",
      "Epoch 129/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5162 - accuracy: 0.3541 - val_loss: 1.2567 - val_accuracy: 0.5036\n",
      "Epoch 130/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.5339 - accuracy: 0.3833 - val_loss: 1.2724 - val_accuracy: 0.4964\n",
      "Epoch 131/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.5273 - accuracy: 0.4273 - val_loss: 1.2731 - val_accuracy: 0.5324\n",
      "Epoch 132/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4761 - accuracy: 0.4080 - val_loss: 1.2542 - val_accuracy: 0.5396\n",
      "Epoch 133/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5594 - accuracy: 0.3684 - val_loss: 1.2459 - val_accuracy: 0.5252\n",
      "Epoch 134/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.5172 - accuracy: 0.4290 - val_loss: 1.2386 - val_accuracy: 0.5468\n",
      "Epoch 135/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4193 - accuracy: 0.4597 - val_loss: 1.2421 - val_accuracy: 0.5108\n",
      "Epoch 136/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.4414 - accuracy: 0.4005 - val_loss: 1.2417 - val_accuracy: 0.5036\n",
      "Epoch 137/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.3868 - accuracy: 0.4572 - val_loss: 1.2252 - val_accuracy: 0.5540\n",
      "Epoch 138/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4090 - accuracy: 0.4514 - val_loss: 1.2374 - val_accuracy: 0.5540\n",
      "Epoch 139/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4682 - accuracy: 0.4189 - val_loss: 1.2629 - val_accuracy: 0.5324\n",
      "Epoch 140/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4788 - accuracy: 0.4023 - val_loss: 1.2325 - val_accuracy: 0.4964\n",
      "Epoch 141/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.3987 - accuracy: 0.4692 - val_loss: 1.2416 - val_accuracy: 0.4748\n",
      "Epoch 142/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.5111 - accuracy: 0.3959 - val_loss: 1.3226 - val_accuracy: 0.4604\n",
      "Epoch 143/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.5149 - accuracy: 0.3576 - val_loss: 1.2213 - val_accuracy: 0.5180\n",
      "Epoch 144/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.3217 - accuracy: 0.4319 - val_loss: 1.1969 - val_accuracy: 0.5396\n",
      "Epoch 145/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.4343 - accuracy: 0.4253 - val_loss: 1.2046 - val_accuracy: 0.5396\n",
      "Epoch 146/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.5239 - accuracy: 0.3898 - val_loss: 1.2284 - val_accuracy: 0.5180\n",
      "Epoch 147/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.4165 - accuracy: 0.4516 - val_loss: 1.2506 - val_accuracy: 0.5180\n",
      "Epoch 148/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.5159 - accuracy: 0.4185 - val_loss: 1.2857 - val_accuracy: 0.4964\n",
      "Epoch 149/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.3072 - accuracy: 0.4982 - val_loss: 1.2152 - val_accuracy: 0.5180\n",
      "Epoch 150/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4807 - accuracy: 0.4816 - val_loss: 1.2191 - val_accuracy: 0.5468\n",
      "Epoch 151/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.5702 - accuracy: 0.3748 - val_loss: 1.2075 - val_accuracy: 0.5540\n",
      "Epoch 152/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4330 - accuracy: 0.4233 - val_loss: 1.2285 - val_accuracy: 0.4964\n",
      "Epoch 153/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.4299 - accuracy: 0.4400 - val_loss: 1.2110 - val_accuracy: 0.5252\n",
      "Epoch 154/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4304 - accuracy: 0.4030 - val_loss: 1.2148 - val_accuracy: 0.5540\n",
      "Epoch 155/1000\n",
      "18/18 [==============================] - 147s 8s/step - loss: 1.5321 - accuracy: 0.4020 - val_loss: 1.3046 - val_accuracy: 0.5396\n",
      "Epoch 156/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.4531 - accuracy: 0.4600 - val_loss: 1.1673 - val_accuracy: 0.5612\n",
      "Epoch 157/1000\n",
      "18/18 [==============================] - 134s 7s/step - loss: 1.4247 - accuracy: 0.3855 - val_loss: 1.1550 - val_accuracy: 0.5396\n",
      "Epoch 158/1000\n",
      "18/18 [==============================] - 143s 8s/step - loss: 1.4598 - accuracy: 0.3942 - val_loss: 1.2159 - val_accuracy: 0.5108\n",
      "Epoch 159/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.3890 - accuracy: 0.4011 - val_loss: 1.1690 - val_accuracy: 0.5324\n",
      "Epoch 160/1000\n",
      "18/18 [==============================] - 121s 7s/step - loss: 1.4130 - accuracy: 0.4225 - val_loss: 1.1432 - val_accuracy: 0.5468\n",
      "Epoch 161/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.4079 - accuracy: 0.4882 - val_loss: 1.2024 - val_accuracy: 0.5252\n",
      "Epoch 162/1000\n",
      "18/18 [==============================] - 156s 9s/step - loss: 1.4422 - accuracy: 0.4640 - val_loss: 1.1502 - val_accuracy: 0.4964\n",
      "Epoch 163/1000\n",
      "18/18 [==============================] - 145s 8s/step - loss: 1.3665 - accuracy: 0.5086 - val_loss: 1.1630 - val_accuracy: 0.5324\n",
      "Epoch 164/1000\n",
      "18/18 [==============================] - 146s 8s/step - loss: 1.2958 - accuracy: 0.5263 - val_loss: 1.1583 - val_accuracy: 0.5324\n",
      "Epoch 165/1000\n",
      "18/18 [==============================] - 156s 9s/step - loss: 1.3830 - accuracy: 0.4699 - val_loss: 1.1795 - val_accuracy: 0.5324\n",
      "Epoch 166/1000\n",
      "18/18 [==============================] - 138s 8s/step - loss: 1.4350 - accuracy: 0.4415 - val_loss: 1.1761 - val_accuracy: 0.5755\n",
      "Epoch 167/1000\n",
      "18/18 [==============================] - 136s 8s/step - loss: 1.2750 - accuracy: 0.5029 - val_loss: 1.1480 - val_accuracy: 0.5612\n",
      "Epoch 168/1000\n",
      "18/18 [==============================] - 133s 7s/step - loss: 1.4213 - accuracy: 0.4881 - val_loss: 1.1518 - val_accuracy: 0.5468\n",
      "Epoch 169/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.5078 - accuracy: 0.3646 - val_loss: 1.1698 - val_accuracy: 0.5540\n",
      "Epoch 170/1000\n",
      "18/18 [==============================] - 156s 9s/step - loss: 1.2506 - accuracy: 0.4938 - val_loss: 1.1779 - val_accuracy: 0.5827\n",
      "Epoch 171/1000\n",
      "18/18 [==============================] - 154s 9s/step - loss: 1.3219 - accuracy: 0.4811 - val_loss: 1.2198 - val_accuracy: 0.5324\n",
      "Epoch 172/1000\n",
      "18/18 [==============================] - 151s 8s/step - loss: 1.3478 - accuracy: 0.4581 - val_loss: 1.2135 - val_accuracy: 0.5468\n",
      "Epoch 173/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 1.4069 - accuracy: 0.4961 - val_loss: 1.2192 - val_accuracy: 0.5827\n",
      "Epoch 174/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.3687 - accuracy: 0.4685 - val_loss: 1.1982 - val_accuracy: 0.5540\n",
      "Epoch 175/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.4790 - accuracy: 0.3985 - val_loss: 1.2088 - val_accuracy: 0.5971\n",
      "Epoch 176/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.3000 - accuracy: 0.5054 - val_loss: 1.2272 - val_accuracy: 0.5612\n",
      "Epoch 177/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.3149 - accuracy: 0.4569 - val_loss: 1.2730 - val_accuracy: 0.5468\n",
      "Epoch 178/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.4383 - accuracy: 0.3708 - val_loss: 1.2319 - val_accuracy: 0.5540\n",
      "Epoch 179/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.3194 - accuracy: 0.4852 - val_loss: 1.3282 - val_accuracy: 0.5252\n",
      "Epoch 180/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.5368 - accuracy: 0.3993 - val_loss: 1.2199 - val_accuracy: 0.5252\n",
      "Epoch 181/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.3449 - accuracy: 0.5243 - val_loss: 1.2055 - val_accuracy: 0.5540\n",
      "Epoch 182/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.5100 - accuracy: 0.4418 - val_loss: 1.2159 - val_accuracy: 0.5540\n",
      "Epoch 183/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.3471 - accuracy: 0.4397 - val_loss: 1.2225 - val_accuracy: 0.5180\n",
      "Epoch 184/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.4141 - accuracy: 0.4642 - val_loss: 1.1668 - val_accuracy: 0.5396\n",
      "Epoch 185/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.3619 - accuracy: 0.5089 - val_loss: 1.1676 - val_accuracy: 0.5540\n",
      "Epoch 186/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.2692 - accuracy: 0.5122 - val_loss: 1.2110 - val_accuracy: 0.5899\n",
      "Epoch 187/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.3756 - accuracy: 0.4537 - val_loss: 1.1862 - val_accuracy: 0.5683\n",
      "Epoch 188/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.3968 - accuracy: 0.4470 - val_loss: 1.2142 - val_accuracy: 0.5540\n",
      "Epoch 189/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.3462 - accuracy: 0.5372 - val_loss: 1.4395 - val_accuracy: 0.5468\n",
      "Epoch 190/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.3983 - accuracy: 0.4577 - val_loss: 1.1881 - val_accuracy: 0.5180\n",
      "Epoch 191/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.3533 - accuracy: 0.5360 - val_loss: 1.1809 - val_accuracy: 0.5540\n",
      "Epoch 192/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.4875 - accuracy: 0.4703 - val_loss: 1.1666 - val_accuracy: 0.5540\n",
      "Epoch 193/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.4376 - accuracy: 0.4221 - val_loss: 1.1430 - val_accuracy: 0.5252\n",
      "Epoch 194/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3473 - accuracy: 0.4048 - val_loss: 1.1406 - val_accuracy: 0.5540\n",
      "Epoch 195/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.4531 - accuracy: 0.4237 - val_loss: 1.2291 - val_accuracy: 0.5540\n",
      "Epoch 196/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.5396 - accuracy: 0.3914 - val_loss: 1.1608 - val_accuracy: 0.5540\n",
      "Epoch 197/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.3405 - accuracy: 0.4717 - val_loss: 1.1732 - val_accuracy: 0.5468\n",
      "Epoch 198/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.3401 - accuracy: 0.4607 - val_loss: 1.1915 - val_accuracy: 0.5827\n",
      "Epoch 199/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.5327 - accuracy: 0.4138 - val_loss: 1.1659 - val_accuracy: 0.5612\n",
      "Epoch 200/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3354 - accuracy: 0.4940 - val_loss: 1.1375 - val_accuracy: 0.5683\n",
      "Epoch 201/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.2798 - accuracy: 0.5138 - val_loss: 1.1728 - val_accuracy: 0.5683\n",
      "Epoch 202/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.3948 - accuracy: 0.4899 - val_loss: 1.2239 - val_accuracy: 0.5827\n",
      "Epoch 203/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.3792 - accuracy: 0.4245 - val_loss: 1.1956 - val_accuracy: 0.5612\n",
      "Epoch 204/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3831 - accuracy: 0.4366 - val_loss: 1.2223 - val_accuracy: 0.5612\n",
      "Epoch 205/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3532 - accuracy: 0.4441 - val_loss: 1.2330 - val_accuracy: 0.5396\n",
      "Epoch 206/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.3598 - accuracy: 0.4529 - val_loss: 1.1968 - val_accuracy: 0.5036\n",
      "Epoch 207/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.2590 - accuracy: 0.5125 - val_loss: 1.1644 - val_accuracy: 0.5468\n",
      "Epoch 208/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.3389 - accuracy: 0.4631 - val_loss: 1.1610 - val_accuracy: 0.5683\n",
      "Epoch 209/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.1998 - accuracy: 0.5175 - val_loss: 1.1610 - val_accuracy: 0.5396\n",
      "Epoch 210/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.2406 - accuracy: 0.4907 - val_loss: 1.1887 - val_accuracy: 0.5612\n",
      "Epoch 211/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.2338 - accuracy: 0.5315 - val_loss: 1.1572 - val_accuracy: 0.5755\n",
      "Epoch 212/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.3481 - accuracy: 0.5127 - val_loss: 1.1708 - val_accuracy: 0.5540\n",
      "Epoch 213/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.5236 - accuracy: 0.4066 - val_loss: 1.2082 - val_accuracy: 0.5396\n",
      "Epoch 214/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.3410 - accuracy: 0.4420 - val_loss: 1.1754 - val_accuracy: 0.5755\n",
      "Epoch 215/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.3021 - accuracy: 0.5138 - val_loss: 1.1258 - val_accuracy: 0.6043\n",
      "Epoch 216/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.2952 - accuracy: 0.5338 - val_loss: 1.1387 - val_accuracy: 0.5827\n",
      "Epoch 217/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.1676 - accuracy: 0.5389 - val_loss: 1.1465 - val_accuracy: 0.5612\n",
      "Epoch 218/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.2632 - accuracy: 0.4842 - val_loss: 1.2824 - val_accuracy: 0.5396\n",
      "Epoch 219/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.4474 - accuracy: 0.4423 - val_loss: 1.2210 - val_accuracy: 0.5324\n",
      "Epoch 220/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.3298 - accuracy: 0.4762 - val_loss: 1.0996 - val_accuracy: 0.6187\n",
      "Epoch 221/1000\n",
      "18/18 [==============================] - 121s 7s/step - loss: 1.4079 - accuracy: 0.4892 - val_loss: 1.0786 - val_accuracy: 0.5899\n",
      "Epoch 222/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.3080 - accuracy: 0.5505 - val_loss: 1.1000 - val_accuracy: 0.6259\n",
      "Epoch 223/1000\n",
      "18/18 [==============================] - 120s 7s/step - loss: 1.3027 - accuracy: 0.5060 - val_loss: 1.1494 - val_accuracy: 0.6115\n",
      "Epoch 224/1000\n",
      "18/18 [==============================] - 148s 8s/step - loss: 1.3909 - accuracy: 0.4611 - val_loss: 1.1582 - val_accuracy: 0.5755\n",
      "Epoch 225/1000\n",
      "18/18 [==============================] - 137s 8s/step - loss: 1.4840 - accuracy: 0.4200 - val_loss: 1.2175 - val_accuracy: 0.6043\n",
      "Epoch 226/1000\n",
      "18/18 [==============================] - 147s 8s/step - loss: 1.2983 - accuracy: 0.4788 - val_loss: 1.1175 - val_accuracy: 0.6043\n",
      "Epoch 227/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 131s 7s/step - loss: 1.3544 - accuracy: 0.4182 - val_loss: 1.1384 - val_accuracy: 0.6187\n",
      "Epoch 228/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.2358 - accuracy: 0.5057 - val_loss: 1.1345 - val_accuracy: 0.6043\n",
      "Epoch 229/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.3024 - accuracy: 0.4966 - val_loss: 1.1427 - val_accuracy: 0.5899\n",
      "Epoch 230/1000\n",
      "18/18 [==============================] - 121s 7s/step - loss: 1.2620 - accuracy: 0.4917 - val_loss: 1.2546 - val_accuracy: 0.5683\n",
      "Epoch 231/1000\n",
      "18/18 [==============================] - 123s 7s/step - loss: 1.2813 - accuracy: 0.5414 - val_loss: 1.1250 - val_accuracy: 0.5468\n",
      "Epoch 232/1000\n",
      "18/18 [==============================] - 140s 8s/step - loss: 1.3008 - accuracy: 0.4710 - val_loss: 1.1258 - val_accuracy: 0.5899\n",
      "Epoch 233/1000\n",
      "18/18 [==============================] - 143s 8s/step - loss: 1.3085 - accuracy: 0.5130 - val_loss: 1.1383 - val_accuracy: 0.5683\n",
      "Epoch 234/1000\n",
      "18/18 [==============================] - 145s 8s/step - loss: 1.3292 - accuracy: 0.4340 - val_loss: 1.1525 - val_accuracy: 0.5971\n",
      "Epoch 235/1000\n",
      "18/18 [==============================] - 144s 8s/step - loss: 1.3313 - accuracy: 0.4548 - val_loss: 1.1553 - val_accuracy: 0.5540\n",
      "Epoch 236/1000\n",
      "18/18 [==============================] - 136s 7s/step - loss: 1.3416 - accuracy: 0.4668 - val_loss: 1.1320 - val_accuracy: 0.5468\n",
      "Epoch 237/1000\n",
      "18/18 [==============================] - 146s 8s/step - loss: 1.3156 - accuracy: 0.4887 - val_loss: 1.1052 - val_accuracy: 0.6043\n",
      "Epoch 238/1000\n",
      "18/18 [==============================] - 140s 8s/step - loss: 1.2797 - accuracy: 0.4824 - val_loss: 1.0966 - val_accuracy: 0.6259\n",
      "Epoch 239/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 1.4475 - accuracy: 0.4476 - val_loss: 1.1530 - val_accuracy: 0.6115\n",
      "Epoch 240/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.3034 - accuracy: 0.4580 - val_loss: 1.1701 - val_accuracy: 0.6043\n",
      "Epoch 241/1000\n",
      "18/18 [==============================] - 135s 7s/step - loss: 1.2684 - accuracy: 0.5038 - val_loss: 1.1587 - val_accuracy: 0.6043\n",
      "Epoch 242/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.3020 - accuracy: 0.5264 - val_loss: 1.1227 - val_accuracy: 0.6115\n",
      "Epoch 243/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.3763 - accuracy: 0.4299 - val_loss: 1.1173 - val_accuracy: 0.6331\n",
      "Epoch 244/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.3103 - accuracy: 0.4905 - val_loss: 1.0797 - val_accuracy: 0.6259\n",
      "Epoch 245/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.4662 - accuracy: 0.4283 - val_loss: 1.0822 - val_accuracy: 0.6043\n",
      "Epoch 246/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.3629 - accuracy: 0.5151 - val_loss: 1.1186 - val_accuracy: 0.6043\n",
      "Epoch 247/1000\n",
      "18/18 [==============================] - 123s 7s/step - loss: 1.2696 - accuracy: 0.4884 - val_loss: 1.0616 - val_accuracy: 0.5683\n",
      "Epoch 248/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.1766 - accuracy: 0.5746 - val_loss: 1.0883 - val_accuracy: 0.5971\n",
      "Epoch 249/1000\n",
      "18/18 [==============================] - 122s 7s/step - loss: 1.2894 - accuracy: 0.5053 - val_loss: 1.1398 - val_accuracy: 0.5827\n",
      "Epoch 250/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.4204 - accuracy: 0.4598 - val_loss: 1.1126 - val_accuracy: 0.5827\n",
      "Epoch 251/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.3083 - accuracy: 0.4722 - val_loss: 1.1232 - val_accuracy: 0.5540\n",
      "Epoch 252/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.2043 - accuracy: 0.5633 - val_loss: 1.1009 - val_accuracy: 0.5827\n",
      "Epoch 253/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.2734 - accuracy: 0.4874 - val_loss: 1.1356 - val_accuracy: 0.5971\n",
      "Epoch 254/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.3315 - accuracy: 0.4697 - val_loss: 1.0946 - val_accuracy: 0.5324\n",
      "Epoch 255/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.2558 - accuracy: 0.4841 - val_loss: 1.1110 - val_accuracy: 0.5612\n",
      "Epoch 256/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.3167 - accuracy: 0.4669 - val_loss: 1.1748 - val_accuracy: 0.5252\n",
      "Epoch 257/1000\n",
      "18/18 [==============================] - 124s 7s/step - loss: 1.4054 - accuracy: 0.4368 - val_loss: 1.0895 - val_accuracy: 0.5971\n",
      "Epoch 258/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.4257 - accuracy: 0.4677 - val_loss: 1.0920 - val_accuracy: 0.5899\n",
      "Epoch 259/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.2485 - accuracy: 0.5085 - val_loss: 1.0913 - val_accuracy: 0.6043\n",
      "Epoch 260/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.2936 - accuracy: 0.5018 - val_loss: 1.0823 - val_accuracy: 0.5755\n",
      "Epoch 261/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.2826 - accuracy: 0.4631 - val_loss: 1.1285 - val_accuracy: 0.6187\n",
      "Epoch 262/1000\n",
      "18/18 [==============================] - 123s 7s/step - loss: 1.3421 - accuracy: 0.5091 - val_loss: 1.0774 - val_accuracy: 0.6115\n",
      "Epoch 263/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.1854 - accuracy: 0.5115 - val_loss: 1.1135 - val_accuracy: 0.5755\n",
      "Epoch 264/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.3594 - accuracy: 0.4468 - val_loss: 1.0983 - val_accuracy: 0.6259\n",
      "Epoch 265/1000\n",
      "18/18 [==============================] - 116s 7s/step - loss: 1.2817 - accuracy: 0.4899 - val_loss: 1.0690 - val_accuracy: 0.6043\n",
      "Epoch 266/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.2367 - accuracy: 0.5180 - val_loss: 1.1528 - val_accuracy: 0.5899\n",
      "Epoch 267/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.3732 - accuracy: 0.4489 - val_loss: 1.0861 - val_accuracy: 0.6043\n",
      "Epoch 268/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.3208 - accuracy: 0.4970 - val_loss: 1.1027 - val_accuracy: 0.6259\n",
      "Epoch 269/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.3629 - accuracy: 0.4657 - val_loss: 1.1450 - val_accuracy: 0.6115\n",
      "Epoch 270/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.3167 - accuracy: 0.4655 - val_loss: 1.2322 - val_accuracy: 0.5971\n",
      "Epoch 271/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.3042 - accuracy: 0.4762 - val_loss: 1.1163 - val_accuracy: 0.6115\n",
      "Epoch 272/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.2508 - accuracy: 0.4993 - val_loss: 1.1514 - val_accuracy: 0.6043\n",
      "Epoch 273/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.2302 - accuracy: 0.4832 - val_loss: 1.1057 - val_accuracy: 0.6043\n",
      "Epoch 274/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 1.1853 - accuracy: 0.5548 - val_loss: 1.1010 - val_accuracy: 0.5755\n",
      "Epoch 275/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.3673 - accuracy: 0.4471 - val_loss: 1.1775 - val_accuracy: 0.5827\n",
      "Epoch 276/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.3238 - accuracy: 0.4556 - val_loss: 1.1105 - val_accuracy: 0.6331\n",
      "Epoch 277/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.2608 - accuracy: 0.5197 - val_loss: 1.1123 - val_accuracy: 0.5971\n",
      "Epoch 278/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.3792 - accuracy: 0.4471 - val_loss: 1.0588 - val_accuracy: 0.6115\n",
      "Epoch 279/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.2107 - accuracy: 0.4943 - val_loss: 1.0666 - val_accuracy: 0.6043\n",
      "Epoch 280/1000\n",
      "18/18 [==============================] - 121s 7s/step - loss: 1.1583 - accuracy: 0.5526 - val_loss: 1.0546 - val_accuracy: 0.6043\n",
      "Epoch 281/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.4154 - accuracy: 0.4993 - val_loss: 1.0757 - val_accuracy: 0.6043\n",
      "Epoch 282/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.4152 - accuracy: 0.4878 - val_loss: 1.0570 - val_accuracy: 0.5899\n",
      "Epoch 283/1000\n",
      "18/18 [==============================] - 122s 7s/step - loss: 1.2281 - accuracy: 0.5113 - val_loss: 1.0586 - val_accuracy: 0.5899\n",
      "Epoch 284/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.1679 - accuracy: 0.5579 - val_loss: 1.0714 - val_accuracy: 0.6115\n",
      "Epoch 285/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 1.2603 - accuracy: 0.5159 - val_loss: 1.0888 - val_accuracy: 0.6331\n",
      "Epoch 286/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.3386 - accuracy: 0.4577 - val_loss: 1.2828 - val_accuracy: 0.6259\n",
      "Epoch 287/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.3207 - accuracy: 0.4157 - val_loss: 1.0981 - val_accuracy: 0.6259\n",
      "Epoch 288/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 1.2392 - accuracy: 0.5177 - val_loss: 1.0832 - val_accuracy: 0.6187\n",
      "Epoch 289/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.1555 - accuracy: 0.5298 - val_loss: 1.0897 - val_accuracy: 0.6043\n",
      "Epoch 290/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 1.2186 - accuracy: 0.5162 - val_loss: 1.1105 - val_accuracy: 0.6115\n",
      "Epoch 291/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 1.1872 - accuracy: 0.5205 - val_loss: 1.0865 - val_accuracy: 0.5755\n",
      "Epoch 292/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.2274 - accuracy: 0.5303 - val_loss: 1.0386 - val_accuracy: 0.6115\n",
      "Epoch 293/1000\n",
      "18/18 [==============================] - 143s 8s/step - loss: 1.1673 - accuracy: 0.5183 - val_loss: 1.0278 - val_accuracy: 0.5971\n",
      "Epoch 294/1000\n",
      "18/18 [==============================] - 135s 8s/step - loss: 1.2785 - accuracy: 0.4687 - val_loss: 1.0127 - val_accuracy: 0.6259\n",
      "Epoch 295/1000\n",
      "18/18 [==============================] - 144s 8s/step - loss: 1.2405 - accuracy: 0.5044 - val_loss: 1.0449 - val_accuracy: 0.6115\n",
      "Epoch 296/1000\n",
      "18/18 [==============================] - 150s 8s/step - loss: 1.2596 - accuracy: 0.5406 - val_loss: 1.1163 - val_accuracy: 0.6043\n",
      "Epoch 297/1000\n",
      "18/18 [==============================] - 170s 9s/step - loss: 1.2220 - accuracy: 0.5081 - val_loss: 1.1075 - val_accuracy: 0.5899\n",
      "Epoch 298/1000\n",
      "18/18 [==============================] - 158s 9s/step - loss: 1.2868 - accuracy: 0.5104 - val_loss: 1.1108 - val_accuracy: 0.5683\n",
      "Epoch 299/1000\n",
      "18/18 [==============================] - 161s 9s/step - loss: 1.3654 - accuracy: 0.4736 - val_loss: 1.1536 - val_accuracy: 0.5755\n",
      "Epoch 300/1000\n",
      "18/18 [==============================] - 166s 9s/step - loss: 1.2714 - accuracy: 0.5089 - val_loss: 1.0949 - val_accuracy: 0.6259\n",
      "Epoch 301/1000\n",
      "18/18 [==============================] - 180s 10s/step - loss: 1.3945 - accuracy: 0.4180 - val_loss: 1.0952 - val_accuracy: 0.6187\n",
      "Epoch 302/1000\n",
      "18/18 [==============================] - 175s 10s/step - loss: 1.3128 - accuracy: 0.4831 - val_loss: 1.1095 - val_accuracy: 0.6187\n",
      "Epoch 303/1000\n",
      "18/18 [==============================] - 178s 10s/step - loss: 1.2128 - accuracy: 0.4946 - val_loss: 1.0788 - val_accuracy: 0.6043\n",
      "Epoch 304/1000\n",
      "18/18 [==============================] - 191s 11s/step - loss: 1.1812 - accuracy: 0.5065 - val_loss: 1.0982 - val_accuracy: 0.5899\n",
      "Epoch 305/1000\n",
      "18/18 [==============================] - 181s 10s/step - loss: 1.2215 - accuracy: 0.5467 - val_loss: 1.0740 - val_accuracy: 0.6187\n",
      "Epoch 306/1000\n",
      "18/18 [==============================] - 210s 12s/step - loss: 1.3156 - accuracy: 0.4850 - val_loss: 1.0798 - val_accuracy: 0.6187\n",
      "Epoch 307/1000\n",
      "18/18 [==============================] - 154s 8s/step - loss: 1.1172 - accuracy: 0.5998 - val_loss: 1.0840 - val_accuracy: 0.5899\n",
      "Epoch 308/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.2355 - accuracy: 0.5686 - val_loss: 1.0680 - val_accuracy: 0.5899\n",
      "Epoch 309/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 1.2109 - accuracy: 0.5312 - val_loss: 1.0498 - val_accuracy: 0.6187\n",
      "Epoch 310/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.2133 - accuracy: 0.5379 - val_loss: 1.0436 - val_accuracy: 0.6115\n",
      "Epoch 311/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 1.2175 - accuracy: 0.5093 - val_loss: 1.0271 - val_accuracy: 0.6259\n",
      "Epoch 312/1000\n",
      "18/18 [==============================] - 135s 8s/step - loss: 1.2059 - accuracy: 0.5387 - val_loss: 1.0185 - val_accuracy: 0.6475\n",
      "Epoch 313/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.1711 - accuracy: 0.5267 - val_loss: 1.0302 - val_accuracy: 0.6115\n",
      "Epoch 314/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.0964 - accuracy: 0.5733 - val_loss: 1.0468 - val_accuracy: 0.6331\n",
      "Epoch 315/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.3441 - accuracy: 0.5064 - val_loss: 1.0567 - val_accuracy: 0.6547\n",
      "Epoch 316/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.1511 - accuracy: 0.5467 - val_loss: 1.0912 - val_accuracy: 0.6547\n",
      "Epoch 317/1000\n",
      "18/18 [==============================] - 135s 7s/step - loss: 1.1271 - accuracy: 0.5385 - val_loss: 1.0558 - val_accuracy: 0.6259\n",
      "Epoch 318/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.2307 - accuracy: 0.5186 - val_loss: 1.0256 - val_accuracy: 0.6331\n",
      "Epoch 319/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.1194 - accuracy: 0.5483 - val_loss: 1.0833 - val_accuracy: 0.6043\n",
      "Epoch 320/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.2042 - accuracy: 0.4769 - val_loss: 1.1011 - val_accuracy: 0.5755\n",
      "Epoch 321/1000\n",
      "18/18 [==============================] - 141s 8s/step - loss: 1.2622 - accuracy: 0.4948 - val_loss: 1.0357 - val_accuracy: 0.6259\n",
      "Epoch 322/1000\n",
      "18/18 [==============================] - 129s 7s/step - loss: 1.1430 - accuracy: 0.5218 - val_loss: 1.0692 - val_accuracy: 0.6187\n",
      "Epoch 323/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.3177 - accuracy: 0.4878 - val_loss: 1.0426 - val_accuracy: 0.6331\n",
      "Epoch 324/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.2405 - accuracy: 0.5782 - val_loss: 1.0549 - val_accuracy: 0.5899\n",
      "Epoch 325/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.3003 - accuracy: 0.5252 - val_loss: 1.0443 - val_accuracy: 0.6187\n",
      "Epoch 326/1000\n",
      "18/18 [==============================] - 138s 8s/step - loss: 1.1811 - accuracy: 0.5267 - val_loss: 1.0385 - val_accuracy: 0.6115\n",
      "Epoch 327/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.2262 - accuracy: 0.5107 - val_loss: 1.0896 - val_accuracy: 0.6043\n",
      "Epoch 328/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.1900 - accuracy: 0.5461 - val_loss: 1.1363 - val_accuracy: 0.5899\n",
      "Epoch 329/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.2369 - accuracy: 0.4827 - val_loss: 1.0551 - val_accuracy: 0.6043\n",
      "Epoch 330/1000\n",
      "18/18 [==============================] - 137s 8s/step - loss: 1.1783 - accuracy: 0.5493 - val_loss: 1.0408 - val_accuracy: 0.6115\n",
      "Epoch 331/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 1.0943 - accuracy: 0.6002 - val_loss: 1.0301 - val_accuracy: 0.6187\n",
      "Epoch 332/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.2279 - accuracy: 0.5136 - val_loss: 1.0405 - val_accuracy: 0.6187\n",
      "Epoch 333/1000\n",
      "18/18 [==============================] - 146s 8s/step - loss: 1.1834 - accuracy: 0.5431 - val_loss: 1.0379 - val_accuracy: 0.5971\n",
      "Epoch 334/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 1.2368 - accuracy: 0.5135 - val_loss: 1.0152 - val_accuracy: 0.6331\n",
      "Epoch 335/1000\n",
      "18/18 [==============================] - 145s 8s/step - loss: 1.2240 - accuracy: 0.5679 - val_loss: 1.0271 - val_accuracy: 0.6331\n",
      "Epoch 336/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.1598 - accuracy: 0.5185 - val_loss: 1.0654 - val_accuracy: 0.6259\n",
      "Epoch 337/1000\n",
      "18/18 [==============================] - 132s 7s/step - loss: 1.2517 - accuracy: 0.5074 - val_loss: 1.0663 - val_accuracy: 0.6115\n",
      "Epoch 338/1000\n",
      "18/18 [==============================] - 125s 7s/step - loss: 1.1500 - accuracy: 0.5720 - val_loss: 1.0963 - val_accuracy: 0.5971\n",
      "Epoch 339/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 111s 6s/step - loss: 1.2060 - accuracy: 0.5576 - val_loss: 1.0761 - val_accuracy: 0.5971\n",
      "Epoch 340/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1873 - accuracy: 0.5461 - val_loss: 1.1061 - val_accuracy: 0.6115\n",
      "Epoch 341/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1444 - accuracy: 0.5346 - val_loss: 1.0419 - val_accuracy: 0.6259\n",
      "Epoch 342/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1456 - accuracy: 0.5607 - val_loss: 1.0578 - val_accuracy: 0.6403\n",
      "Epoch 343/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2613 - accuracy: 0.5385 - val_loss: 1.1033 - val_accuracy: 0.6475\n",
      "Epoch 344/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.3239 - accuracy: 0.4997 - val_loss: 1.1020 - val_accuracy: 0.6259\n",
      "Epoch 345/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.1714 - accuracy: 0.5439 - val_loss: 1.1041 - val_accuracy: 0.5899\n",
      "Epoch 346/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2942 - accuracy: 0.4987 - val_loss: 1.1217 - val_accuracy: 0.5971\n",
      "Epoch 347/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1552 - accuracy: 0.5399 - val_loss: 1.1416 - val_accuracy: 0.6115\n",
      "Epoch 348/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2071 - accuracy: 0.5402 - val_loss: 1.1187 - val_accuracy: 0.6187\n",
      "Epoch 349/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1671 - accuracy: 0.5288 - val_loss: 1.1347 - val_accuracy: 0.6475\n",
      "Epoch 350/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2871 - accuracy: 0.5105 - val_loss: 1.0746 - val_accuracy: 0.6403\n",
      "Epoch 351/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.1741 - accuracy: 0.5498 - val_loss: 1.1079 - val_accuracy: 0.6403\n",
      "Epoch 352/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0253 - accuracy: 0.6077 - val_loss: 1.0801 - val_accuracy: 0.6403\n",
      "Epoch 353/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1607 - accuracy: 0.5246 - val_loss: 1.0511 - val_accuracy: 0.6475\n",
      "Epoch 354/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2422 - accuracy: 0.5365 - val_loss: 1.0453 - val_accuracy: 0.6187\n",
      "Epoch 355/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2111 - accuracy: 0.5176 - val_loss: 1.0299 - val_accuracy: 0.6403\n",
      "Epoch 356/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.1674 - accuracy: 0.5389 - val_loss: 1.0376 - val_accuracy: 0.6331\n",
      "Epoch 357/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1168 - accuracy: 0.5256 - val_loss: 1.0442 - val_accuracy: 0.6403\n",
      "Epoch 358/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1957 - accuracy: 0.5368 - val_loss: 1.1104 - val_accuracy: 0.6403\n",
      "Epoch 359/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.1077 - accuracy: 0.6070 - val_loss: 1.0609 - val_accuracy: 0.6619\n",
      "Epoch 360/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1817 - accuracy: 0.5172 - val_loss: 1.0721 - val_accuracy: 0.6619\n",
      "Epoch 361/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1544 - accuracy: 0.4975 - val_loss: 1.0202 - val_accuracy: 0.6403\n",
      "Epoch 362/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.1755 - accuracy: 0.5814 - val_loss: 1.0235 - val_accuracy: 0.6547\n",
      "Epoch 363/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1015 - accuracy: 0.5405 - val_loss: 1.0289 - val_accuracy: 0.6259\n",
      "Epoch 364/1000\n",
      "18/18 [==============================] - 120s 7s/step - loss: 1.1387 - accuracy: 0.5389 - val_loss: 1.0786 - val_accuracy: 0.5827\n",
      "Epoch 365/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.3346 - accuracy: 0.5264 - val_loss: 1.0900 - val_accuracy: 0.6403\n",
      "Epoch 366/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.1543 - accuracy: 0.5872 - val_loss: 1.0462 - val_accuracy: 0.6331\n",
      "Epoch 367/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 1.0370 - accuracy: 0.6027 - val_loss: 1.0383 - val_accuracy: 0.6475\n",
      "Epoch 368/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.1303 - accuracy: 0.5889 - val_loss: 1.0892 - val_accuracy: 0.6259\n",
      "Epoch 369/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1215 - accuracy: 0.5941 - val_loss: 1.0877 - val_accuracy: 0.6115\n",
      "Epoch 370/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.2970 - accuracy: 0.5105 - val_loss: 1.0836 - val_accuracy: 0.6115\n",
      "Epoch 371/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2581 - accuracy: 0.5090 - val_loss: 1.1067 - val_accuracy: 0.6043\n",
      "Epoch 372/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.3458 - accuracy: 0.5198 - val_loss: 1.2305 - val_accuracy: 0.6187\n",
      "Epoch 373/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.2183 - accuracy: 0.4938 - val_loss: 1.1881 - val_accuracy: 0.6259\n",
      "Epoch 374/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.1749 - accuracy: 0.5708 - val_loss: 1.0641 - val_accuracy: 0.6259\n",
      "Epoch 375/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.1343 - accuracy: 0.5942 - val_loss: 1.0929 - val_accuracy: 0.6187\n",
      "Epoch 376/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1824 - accuracy: 0.5465 - val_loss: 1.0427 - val_accuracy: 0.6259\n",
      "Epoch 377/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.2028 - accuracy: 0.5590 - val_loss: 1.0107 - val_accuracy: 0.5971\n",
      "Epoch 378/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1951 - accuracy: 0.5628 - val_loss: 1.0907 - val_accuracy: 0.6259\n",
      "Epoch 379/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1302 - accuracy: 0.5302 - val_loss: 1.0750 - val_accuracy: 0.6475\n",
      "Epoch 380/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.2259 - accuracy: 0.5094 - val_loss: 1.0749 - val_accuracy: 0.5971\n",
      "Epoch 381/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1793 - accuracy: 0.5885 - val_loss: 1.0521 - val_accuracy: 0.6403\n",
      "Epoch 382/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2720 - accuracy: 0.4698 - val_loss: 1.0355 - val_accuracy: 0.6115\n",
      "Epoch 383/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2226 - accuracy: 0.5360 - val_loss: 1.1271 - val_accuracy: 0.6187\n",
      "Epoch 384/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1469 - accuracy: 0.5436 - val_loss: 1.0076 - val_accuracy: 0.6115\n",
      "Epoch 385/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.1114 - accuracy: 0.5957 - val_loss: 1.0273 - val_accuracy: 0.6691\n",
      "Epoch 386/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.3067 - accuracy: 0.5215 - val_loss: 1.0089 - val_accuracy: 0.6115\n",
      "Epoch 387/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2040 - accuracy: 0.5163 - val_loss: 1.1009 - val_accuracy: 0.6547\n",
      "Epoch 388/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2492 - accuracy: 0.4571 - val_loss: 1.0141 - val_accuracy: 0.5971\n",
      "Epoch 389/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1390 - accuracy: 0.5491 - val_loss: 1.0955 - val_accuracy: 0.6475\n",
      "Epoch 390/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1694 - accuracy: 0.5644 - val_loss: 1.0473 - val_accuracy: 0.6475\n",
      "Epoch 391/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1913 - accuracy: 0.5747 - val_loss: 1.0670 - val_accuracy: 0.6403\n",
      "Epoch 392/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1405 - accuracy: 0.5504 - val_loss: 1.0904 - val_accuracy: 0.6403\n",
      "Epoch 393/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1398 - accuracy: 0.5469 - val_loss: 1.0452 - val_accuracy: 0.6403\n",
      "Epoch 394/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.0981 - accuracy: 0.5662 - val_loss: 1.0702 - val_accuracy: 0.6475\n",
      "Epoch 395/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1078 - accuracy: 0.5704 - val_loss: 1.1186 - val_accuracy: 0.6259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2752 - accuracy: 0.5519 - val_loss: 1.0726 - val_accuracy: 0.5899\n",
      "Epoch 397/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1422 - accuracy: 0.5503 - val_loss: 1.0658 - val_accuracy: 0.6043\n",
      "Epoch 398/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1345 - accuracy: 0.5413 - val_loss: 1.0751 - val_accuracy: 0.6403\n",
      "Epoch 399/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1500 - accuracy: 0.5397 - val_loss: 1.1316 - val_accuracy: 0.6043\n",
      "Epoch 400/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2226 - accuracy: 0.5340 - val_loss: 1.1363 - val_accuracy: 0.5971\n",
      "Epoch 401/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0970 - accuracy: 0.5519 - val_loss: 1.1303 - val_accuracy: 0.6115\n",
      "Epoch 402/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1533 - accuracy: 0.5583 - val_loss: 1.1366 - val_accuracy: 0.6115\n",
      "Epoch 403/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.1119 - accuracy: 0.5487 - val_loss: 1.0924 - val_accuracy: 0.6331\n",
      "Epoch 404/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1213 - accuracy: 0.5809 - val_loss: 1.1321 - val_accuracy: 0.6187\n",
      "Epoch 405/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0261 - accuracy: 0.6001 - val_loss: 1.1636 - val_accuracy: 0.6043\n",
      "Epoch 406/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0919 - accuracy: 0.5601 - val_loss: 1.2380 - val_accuracy: 0.6331\n",
      "Epoch 407/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1172 - accuracy: 0.5603 - val_loss: 1.2529 - val_accuracy: 0.6115\n",
      "Epoch 408/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2070 - accuracy: 0.5260 - val_loss: 1.1565 - val_accuracy: 0.6043\n",
      "Epoch 409/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1364 - accuracy: 0.5861 - val_loss: 1.1825 - val_accuracy: 0.6403\n",
      "Epoch 410/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0952 - accuracy: 0.5463 - val_loss: 1.1756 - val_accuracy: 0.6259\n",
      "Epoch 411/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1396 - accuracy: 0.5323 - val_loss: 1.1253 - val_accuracy: 0.5971\n",
      "Epoch 412/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.1017 - accuracy: 0.5935 - val_loss: 1.1381 - val_accuracy: 0.6115\n",
      "Epoch 413/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2084 - accuracy: 0.4622 - val_loss: 1.2586 - val_accuracy: 0.6403\n",
      "Epoch 414/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2682 - accuracy: 0.5149 - val_loss: 1.1220 - val_accuracy: 0.6547\n",
      "Epoch 415/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0314 - accuracy: 0.5769 - val_loss: 1.1481 - val_accuracy: 0.5971\n",
      "Epoch 416/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1033 - accuracy: 0.5427 - val_loss: 1.1423 - val_accuracy: 0.6187\n",
      "Epoch 417/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0292 - accuracy: 0.5754 - val_loss: 1.1785 - val_accuracy: 0.6331\n",
      "Epoch 418/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0868 - accuracy: 0.5567 - val_loss: 1.1456 - val_accuracy: 0.6043\n",
      "Epoch 419/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.1887 - accuracy: 0.5476 - val_loss: 1.2682 - val_accuracy: 0.5971\n",
      "Epoch 420/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2025 - accuracy: 0.5107 - val_loss: 1.1927 - val_accuracy: 0.6331\n",
      "Epoch 421/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.2354 - accuracy: 0.5378 - val_loss: 1.1017 - val_accuracy: 0.6403\n",
      "Epoch 422/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1328 - accuracy: 0.5734 - val_loss: 1.0791 - val_accuracy: 0.6403\n",
      "Epoch 423/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0867 - accuracy: 0.6086 - val_loss: 1.0761 - val_accuracy: 0.6691\n",
      "Epoch 424/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2431 - accuracy: 0.5785 - val_loss: 1.0847 - val_accuracy: 0.6115\n",
      "Epoch 425/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0806 - accuracy: 0.5768 - val_loss: 1.1380 - val_accuracy: 0.6187\n",
      "Epoch 426/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1819 - accuracy: 0.5254 - val_loss: 1.0713 - val_accuracy: 0.6115\n",
      "Epoch 427/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1220 - accuracy: 0.5351 - val_loss: 1.2762 - val_accuracy: 0.6187\n",
      "Epoch 428/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1387 - accuracy: 0.5640 - val_loss: 1.1014 - val_accuracy: 0.6403\n",
      "Epoch 429/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0968 - accuracy: 0.5987 - val_loss: 1.1282 - val_accuracy: 0.6187\n",
      "Epoch 430/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0570 - accuracy: 0.6271 - val_loss: 1.1313 - val_accuracy: 0.5971\n",
      "Epoch 431/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.1768 - accuracy: 0.5190 - val_loss: 1.0747 - val_accuracy: 0.5971\n",
      "Epoch 432/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0460 - accuracy: 0.6092 - val_loss: 1.1071 - val_accuracy: 0.5971\n",
      "Epoch 433/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2228 - accuracy: 0.5388 - val_loss: 1.0735 - val_accuracy: 0.6547\n",
      "Epoch 434/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1747 - accuracy: 0.5814 - val_loss: 1.0570 - val_accuracy: 0.5827\n",
      "Epoch 435/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0752 - accuracy: 0.5809 - val_loss: 1.0939 - val_accuracy: 0.6187\n",
      "Epoch 436/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0271 - accuracy: 0.6554 - val_loss: 1.1504 - val_accuracy: 0.6043\n",
      "Epoch 437/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.9585 - accuracy: 0.5948 - val_loss: 1.1206 - val_accuracy: 0.6331\n",
      "Epoch 438/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0962 - accuracy: 0.6051 - val_loss: 1.1530 - val_accuracy: 0.6187\n",
      "Epoch 439/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0094 - accuracy: 0.5945 - val_loss: 1.1175 - val_accuracy: 0.6187\n",
      "Epoch 440/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1540 - accuracy: 0.5413 - val_loss: 1.0840 - val_accuracy: 0.6187\n",
      "Epoch 441/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0457 - accuracy: 0.5754 - val_loss: 1.1738 - val_accuracy: 0.6187\n",
      "Epoch 442/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.9512 - accuracy: 0.6196 - val_loss: 1.2020 - val_accuracy: 0.6187\n",
      "Epoch 443/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1670 - accuracy: 0.5746 - val_loss: 1.1421 - val_accuracy: 0.6187\n",
      "Epoch 444/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1723 - accuracy: 0.5278 - val_loss: 1.1548 - val_accuracy: 0.6115\n",
      "Epoch 445/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9925 - accuracy: 0.6351 - val_loss: 1.1059 - val_accuracy: 0.6187\n",
      "Epoch 446/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2416 - accuracy: 0.4801 - val_loss: 1.0943 - val_accuracy: 0.6475\n",
      "Epoch 447/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.3131 - accuracy: 0.5100 - val_loss: 1.1701 - val_accuracy: 0.6187\n",
      "Epoch 448/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0956 - accuracy: 0.5733 - val_loss: 1.1059 - val_accuracy: 0.6331\n",
      "Epoch 449/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1586 - accuracy: 0.5516 - val_loss: 1.1886 - val_accuracy: 0.6259\n",
      "Epoch 450/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.0416 - accuracy: 0.5565 - val_loss: 1.1668 - val_accuracy: 0.6043\n",
      "Epoch 451/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1376 - accuracy: 0.5348 - val_loss: 1.1645 - val_accuracy: 0.6043\n",
      "Epoch 452/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0877 - accuracy: 0.5885 - val_loss: 1.1239 - val_accuracy: 0.5827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 453/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.1465 - accuracy: 0.5475 - val_loss: 1.3317 - val_accuracy: 0.6115\n",
      "Epoch 454/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1567 - accuracy: 0.5119 - val_loss: 1.0958 - val_accuracy: 0.6403\n",
      "Epoch 455/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0424 - accuracy: 0.5938 - val_loss: 1.1054 - val_accuracy: 0.6403\n",
      "Epoch 456/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1182 - accuracy: 0.5739 - val_loss: 1.1117 - val_accuracy: 0.6331\n",
      "Epoch 457/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0537 - accuracy: 0.6192 - val_loss: 1.1102 - val_accuracy: 0.6259\n",
      "Epoch 458/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0071 - accuracy: 0.6178 - val_loss: 1.1865 - val_accuracy: 0.5827\n",
      "Epoch 459/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2816 - accuracy: 0.5150 - val_loss: 1.0750 - val_accuracy: 0.6331\n",
      "Epoch 460/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1143 - accuracy: 0.6062 - val_loss: 1.0611 - val_accuracy: 0.6187\n",
      "Epoch 461/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.1641 - accuracy: 0.5573 - val_loss: 1.0959 - val_accuracy: 0.6403\n",
      "Epoch 462/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1777 - accuracy: 0.5584 - val_loss: 1.0431 - val_accuracy: 0.6547\n",
      "Epoch 463/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0635 - accuracy: 0.6079 - val_loss: 1.0497 - val_accuracy: 0.6475\n",
      "Epoch 464/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0243 - accuracy: 0.5957 - val_loss: 1.0741 - val_accuracy: 0.6547\n",
      "Epoch 465/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0775 - accuracy: 0.6044 - val_loss: 1.0470 - val_accuracy: 0.6187\n",
      "Epoch 466/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1209 - accuracy: 0.5568 - val_loss: 1.1169 - val_accuracy: 0.6331\n",
      "Epoch 467/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.3096 - accuracy: 0.5312 - val_loss: 1.0774 - val_accuracy: 0.5899\n",
      "Epoch 468/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0702 - accuracy: 0.5972 - val_loss: 1.0826 - val_accuracy: 0.6043\n",
      "Epoch 469/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2500 - accuracy: 0.5444 - val_loss: 1.0873 - val_accuracy: 0.6403\n",
      "Epoch 470/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0657 - accuracy: 0.5883 - val_loss: 1.1109 - val_accuracy: 0.6259\n",
      "Epoch 471/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0064 - accuracy: 0.6165 - val_loss: 1.0825 - val_accuracy: 0.6331\n",
      "Epoch 472/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1814 - accuracy: 0.5520 - val_loss: 1.0927 - val_accuracy: 0.6043\n",
      "Epoch 473/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0909 - accuracy: 0.5798 - val_loss: 1.1950 - val_accuracy: 0.6115\n",
      "Epoch 474/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.0955 - accuracy: 0.5636 - val_loss: 1.1156 - val_accuracy: 0.6403\n",
      "Epoch 475/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.0705 - accuracy: 0.5929 - val_loss: 1.0797 - val_accuracy: 0.6259\n",
      "Epoch 476/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1545 - accuracy: 0.5235 - val_loss: 1.1391 - val_accuracy: 0.6403\n",
      "Epoch 477/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1352 - accuracy: 0.5739 - val_loss: 1.0825 - val_accuracy: 0.6043\n",
      "Epoch 478/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0606 - accuracy: 0.5890 - val_loss: 1.1364 - val_accuracy: 0.6043\n",
      "Epoch 479/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0510 - accuracy: 0.5943 - val_loss: 1.1107 - val_accuracy: 0.6259\n",
      "Epoch 480/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1659 - accuracy: 0.5601 - val_loss: 1.0897 - val_accuracy: 0.6259\n",
      "Epoch 481/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1644 - accuracy: 0.5447 - val_loss: 1.1717 - val_accuracy: 0.6187\n",
      "Epoch 482/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1349 - accuracy: 0.5834 - val_loss: 1.1003 - val_accuracy: 0.6403\n",
      "Epoch 483/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0268 - accuracy: 0.5937 - val_loss: 1.1305 - val_accuracy: 0.6259\n",
      "Epoch 484/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0431 - accuracy: 0.6377 - val_loss: 1.1025 - val_accuracy: 0.6259\n",
      "Epoch 485/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0852 - accuracy: 0.5773 - val_loss: 1.1001 - val_accuracy: 0.6331\n",
      "Epoch 486/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2603 - accuracy: 0.5650 - val_loss: 1.1358 - val_accuracy: 0.6403\n",
      "Epoch 487/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.1628 - accuracy: 0.5356 - val_loss: 1.1258 - val_accuracy: 0.6331\n",
      "Epoch 488/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1859 - accuracy: 0.5935 - val_loss: 1.1583 - val_accuracy: 0.6403\n",
      "Epoch 489/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.9996 - accuracy: 0.6594 - val_loss: 1.0800 - val_accuracy: 0.5971\n",
      "Epoch 490/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.2240 - accuracy: 0.5484 - val_loss: 1.1254 - val_accuracy: 0.6619\n",
      "Epoch 491/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1463 - accuracy: 0.5772 - val_loss: 1.1160 - val_accuracy: 0.6115\n",
      "Epoch 492/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1514 - accuracy: 0.5738 - val_loss: 1.1058 - val_accuracy: 0.6043\n",
      "Epoch 493/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.0456 - accuracy: 0.6007 - val_loss: 1.0751 - val_accuracy: 0.6331\n",
      "Epoch 494/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0304 - accuracy: 0.5863 - val_loss: 1.0866 - val_accuracy: 0.6043\n",
      "Epoch 495/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0733 - accuracy: 0.5766 - val_loss: 1.0551 - val_accuracy: 0.6259\n",
      "Epoch 496/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1049 - accuracy: 0.5985 - val_loss: 1.0887 - val_accuracy: 0.6259\n",
      "Epoch 497/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1518 - accuracy: 0.5612 - val_loss: 1.0996 - val_accuracy: 0.6115\n",
      "Epoch 498/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0702 - accuracy: 0.6013 - val_loss: 1.0931 - val_accuracy: 0.6259\n",
      "Epoch 499/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.1835 - accuracy: 0.5806 - val_loss: 1.1369 - val_accuracy: 0.6331\n",
      "Epoch 500/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0330 - accuracy: 0.6363 - val_loss: 1.1493 - val_accuracy: 0.6259\n",
      "Epoch 501/1000\n",
      "18/18 [==============================] - 98s 6s/step - loss: 1.0023 - accuracy: 0.6082 - val_loss: 1.0971 - val_accuracy: 0.6475\n",
      "Epoch 502/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0387 - accuracy: 0.6048 - val_loss: 1.1336 - val_accuracy: 0.6547\n",
      "Epoch 503/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9538 - accuracy: 0.6486 - val_loss: 1.1996 - val_accuracy: 0.6403\n",
      "Epoch 504/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.1254 - accuracy: 0.5623 - val_loss: 1.1463 - val_accuracy: 0.6259\n",
      "Epoch 505/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9832 - accuracy: 0.6168 - val_loss: 1.2264 - val_accuracy: 0.5971\n",
      "Epoch 506/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1419 - accuracy: 0.5486 - val_loss: 1.1613 - val_accuracy: 0.6475\n",
      "Epoch 507/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1425 - accuracy: 0.5113 - val_loss: 1.1686 - val_accuracy: 0.6187\n",
      "Epoch 508/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0753 - accuracy: 0.5890 - val_loss: 1.1745 - val_accuracy: 0.6259\n",
      "Epoch 509/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.2198 - accuracy: 0.4841 - val_loss: 1.1706 - val_accuracy: 0.5971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 510/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.9917 - accuracy: 0.6112 - val_loss: 1.3626 - val_accuracy: 0.6043\n",
      "Epoch 511/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 1.1216 - accuracy: 0.6023 - val_loss: 1.1784 - val_accuracy: 0.6115\n",
      "Epoch 512/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 0.9607 - accuracy: 0.6450 - val_loss: 1.2287 - val_accuracy: 0.6547\n",
      "Epoch 513/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1265 - accuracy: 0.5659 - val_loss: 1.1351 - val_accuracy: 0.6475\n",
      "Epoch 514/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0304 - accuracy: 0.6161 - val_loss: 1.1753 - val_accuracy: 0.6331\n",
      "Epoch 515/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1289 - accuracy: 0.5845 - val_loss: 1.1758 - val_accuracy: 0.6259\n",
      "Epoch 516/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1293 - accuracy: 0.5834 - val_loss: 1.1370 - val_accuracy: 0.6115\n",
      "Epoch 517/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.2375 - accuracy: 0.5511 - val_loss: 1.1891 - val_accuracy: 0.6331\n",
      "Epoch 518/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0904 - accuracy: 0.5693 - val_loss: 1.0921 - val_accuracy: 0.6331\n",
      "Epoch 519/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0345 - accuracy: 0.5784 - val_loss: 1.1381 - val_accuracy: 0.6043\n",
      "Epoch 520/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1500 - accuracy: 0.5623 - val_loss: 1.2690 - val_accuracy: 0.5827\n",
      "Epoch 521/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0928 - accuracy: 0.6067 - val_loss: 1.1746 - val_accuracy: 0.6187\n",
      "Epoch 522/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1024 - accuracy: 0.5738 - val_loss: 1.1782 - val_accuracy: 0.6259\n",
      "Epoch 523/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0526 - accuracy: 0.5776 - val_loss: 1.1505 - val_accuracy: 0.6043\n",
      "Epoch 524/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0968 - accuracy: 0.5781 - val_loss: 1.0723 - val_accuracy: 0.6259\n",
      "Epoch 525/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0845 - accuracy: 0.5767 - val_loss: 1.0682 - val_accuracy: 0.6115\n",
      "Epoch 526/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0642 - accuracy: 0.5657 - val_loss: 1.1180 - val_accuracy: 0.6187\n",
      "Epoch 527/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 0.9772 - accuracy: 0.6136 - val_loss: 1.1789 - val_accuracy: 0.6331\n",
      "Epoch 528/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1140 - accuracy: 0.5461 - val_loss: 1.1308 - val_accuracy: 0.6187\n",
      "Epoch 529/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.0408 - accuracy: 0.5739 - val_loss: 1.1812 - val_accuracy: 0.6259\n",
      "Epoch 530/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0098 - accuracy: 0.5627 - val_loss: 1.0799 - val_accuracy: 0.6259\n",
      "Epoch 531/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1151 - accuracy: 0.5297 - val_loss: 1.0341 - val_accuracy: 0.6547\n",
      "Epoch 532/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1357 - accuracy: 0.5375 - val_loss: 1.0406 - val_accuracy: 0.6115\n",
      "Epoch 533/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9898 - accuracy: 0.5954 - val_loss: 1.1033 - val_accuracy: 0.6403\n",
      "Epoch 534/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0774 - accuracy: 0.5842 - val_loss: 1.0760 - val_accuracy: 0.6403\n",
      "Epoch 535/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9326 - accuracy: 0.6739 - val_loss: 1.0574 - val_accuracy: 0.6259\n",
      "Epoch 536/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1420 - accuracy: 0.6095 - val_loss: 0.9763 - val_accuracy: 0.6547\n",
      "Epoch 537/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.1222 - accuracy: 0.5530 - val_loss: 1.1307 - val_accuracy: 0.6619\n",
      "Epoch 538/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.0257 - accuracy: 0.6270 - val_loss: 1.0017 - val_accuracy: 0.6475\n",
      "Epoch 539/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 0.9985 - accuracy: 0.6236 - val_loss: 1.0644 - val_accuracy: 0.6403\n",
      "Epoch 540/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.9793 - accuracy: 0.6206 - val_loss: 1.0124 - val_accuracy: 0.6619\n",
      "Epoch 541/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 1.2038 - accuracy: 0.5632 - val_loss: 0.9924 - val_accuracy: 0.6619\n",
      "Epoch 542/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9283 - accuracy: 0.6098 - val_loss: 1.0431 - val_accuracy: 0.6547\n",
      "Epoch 543/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0478 - accuracy: 0.5675 - val_loss: 1.0412 - val_accuracy: 0.6619\n",
      "Epoch 544/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1214 - accuracy: 0.5346 - val_loss: 1.0087 - val_accuracy: 0.6691\n",
      "Epoch 545/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.1343 - accuracy: 0.5736 - val_loss: 1.0130 - val_accuracy: 0.6619\n",
      "Epoch 546/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0794 - accuracy: 0.6005 - val_loss: 1.0274 - val_accuracy: 0.6547\n",
      "Epoch 547/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.9080 - accuracy: 0.6359 - val_loss: 1.1080 - val_accuracy: 0.6475\n",
      "Epoch 548/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9705 - accuracy: 0.6547 - val_loss: 1.0748 - val_accuracy: 0.6835\n",
      "Epoch 549/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.2536 - accuracy: 0.5522 - val_loss: 1.1386 - val_accuracy: 0.6475\n",
      "Epoch 550/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9103 - accuracy: 0.6195 - val_loss: 1.1318 - val_accuracy: 0.6475\n",
      "Epoch 551/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.8884 - accuracy: 0.6880 - val_loss: 1.0328 - val_accuracy: 0.6619\n",
      "Epoch 552/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9428 - accuracy: 0.6493 - val_loss: 1.0207 - val_accuracy: 0.6763\n",
      "Epoch 553/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0359 - accuracy: 0.6390 - val_loss: 0.9970 - val_accuracy: 0.6547\n",
      "Epoch 554/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.1013 - accuracy: 0.6221 - val_loss: 1.0920 - val_accuracy: 0.6978\n",
      "Epoch 555/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.2045 - accuracy: 0.5699 - val_loss: 1.0716 - val_accuracy: 0.6619\n",
      "Epoch 556/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.1200 - accuracy: 0.5755 - val_loss: 1.1086 - val_accuracy: 0.6906\n",
      "Epoch 557/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9719 - accuracy: 0.6233 - val_loss: 1.1880 - val_accuracy: 0.6475\n",
      "Epoch 558/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.0796 - accuracy: 0.5649 - val_loss: 1.2116 - val_accuracy: 0.6547\n",
      "Epoch 559/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0204 - accuracy: 0.6442 - val_loss: 1.0827 - val_accuracy: 0.6763\n",
      "Epoch 560/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9753 - accuracy: 0.6338 - val_loss: 1.1982 - val_accuracy: 0.6403\n",
      "Epoch 561/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.1797 - accuracy: 0.5366 - val_loss: 1.1246 - val_accuracy: 0.6475\n",
      "Epoch 562/1000\n",
      "18/18 [==============================] - 98s 5s/step - loss: 1.1483 - accuracy: 0.5619 - val_loss: 1.0289 - val_accuracy: 0.6835\n",
      "Epoch 563/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1517 - accuracy: 0.5820 - val_loss: 1.0654 - val_accuracy: 0.6691\n",
      "Epoch 564/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 1.0016 - accuracy: 0.6479 - val_loss: 1.0944 - val_accuracy: 0.6619\n",
      "Epoch 565/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.1080 - accuracy: 0.5212 - val_loss: 1.1169 - val_accuracy: 0.6331\n",
      "Epoch 566/1000\n",
      "18/18 [==============================] - 125s 7s/step - loss: 1.0977 - accuracy: 0.5873 - val_loss: 1.0953 - val_accuracy: 0.6547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.9566 - accuracy: 0.6437 - val_loss: 1.1120 - val_accuracy: 0.6331\n",
      "Epoch 568/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.0459 - accuracy: 0.5633 - val_loss: 1.1169 - val_accuracy: 0.6331\n",
      "Epoch 569/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0668 - accuracy: 0.5876 - val_loss: 1.1271 - val_accuracy: 0.6547\n",
      "Epoch 570/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 1.0150 - accuracy: 0.6471 - val_loss: 1.0811 - val_accuracy: 0.6547\n",
      "Epoch 571/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0060 - accuracy: 0.6174 - val_loss: 1.1512 - val_accuracy: 0.6547\n",
      "Epoch 572/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0861 - accuracy: 0.5651 - val_loss: 1.0552 - val_accuracy: 0.6619\n",
      "Epoch 573/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0794 - accuracy: 0.6058 - val_loss: 1.0912 - val_accuracy: 0.6835\n",
      "Epoch 574/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0666 - accuracy: 0.5759 - val_loss: 1.1281 - val_accuracy: 0.6906\n",
      "Epoch 575/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.9485 - accuracy: 0.5953 - val_loss: 1.1080 - val_accuracy: 0.6403\n",
      "Epoch 576/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.0837 - accuracy: 0.5919 - val_loss: 1.1904 - val_accuracy: 0.6619\n",
      "Epoch 577/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.9831 - accuracy: 0.6197 - val_loss: 1.2181 - val_accuracy: 0.6619\n",
      "Epoch 578/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0612 - accuracy: 0.5981 - val_loss: 1.1559 - val_accuracy: 0.6475\n",
      "Epoch 579/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.9424 - accuracy: 0.6150 - val_loss: 1.2278 - val_accuracy: 0.6691\n",
      "Epoch 580/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0741 - accuracy: 0.5670 - val_loss: 1.0852 - val_accuracy: 0.6403\n",
      "Epoch 581/1000\n",
      "18/18 [==============================] - 116s 7s/step - loss: 0.9921 - accuracy: 0.6422 - val_loss: 1.1125 - val_accuracy: 0.6331\n",
      "Epoch 582/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 1.0634 - accuracy: 0.6198 - val_loss: 1.0947 - val_accuracy: 0.6619\n",
      "Epoch 583/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8923 - accuracy: 0.6823 - val_loss: 1.1353 - val_accuracy: 0.6547\n",
      "Epoch 584/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0514 - accuracy: 0.6157 - val_loss: 1.1788 - val_accuracy: 0.6403\n",
      "Epoch 585/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.1965 - accuracy: 0.5709 - val_loss: 1.1795 - val_accuracy: 0.6403\n",
      "Epoch 586/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0511 - accuracy: 0.6367 - val_loss: 1.1607 - val_accuracy: 0.5827\n",
      "Epoch 587/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.9940 - accuracy: 0.5778 - val_loss: 1.1576 - val_accuracy: 0.6115\n",
      "Epoch 588/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0273 - accuracy: 0.6360 - val_loss: 1.2004 - val_accuracy: 0.6115\n",
      "Epoch 589/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9446 - accuracy: 0.6458 - val_loss: 1.1208 - val_accuracy: 0.6187\n",
      "Epoch 590/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0531 - accuracy: 0.5822 - val_loss: 1.1282 - val_accuracy: 0.6187\n",
      "Epoch 591/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8838 - accuracy: 0.6348 - val_loss: 1.2383 - val_accuracy: 0.6403\n",
      "Epoch 592/1000\n",
      "18/18 [==============================] - 133s 7s/step - loss: 0.9384 - accuracy: 0.6703 - val_loss: 1.1448 - val_accuracy: 0.5827\n",
      "Epoch 593/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 1.0936 - accuracy: 0.6340 - val_loss: 1.1372 - val_accuracy: 0.6043\n",
      "Epoch 594/1000\n",
      "18/18 [==============================] - 145s 8s/step - loss: 0.9452 - accuracy: 0.6568 - val_loss: 1.4752 - val_accuracy: 0.6043\n",
      "Epoch 595/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.9632 - accuracy: 0.6371 - val_loss: 1.0710 - val_accuracy: 0.5971\n",
      "Epoch 596/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.9952 - accuracy: 0.6356 - val_loss: 1.2457 - val_accuracy: 0.6115\n",
      "Epoch 597/1000\n",
      "18/18 [==============================] - 163s 9s/step - loss: 1.1984 - accuracy: 0.5402 - val_loss: 1.1199 - val_accuracy: 0.6043\n",
      "Epoch 598/1000\n",
      "18/18 [==============================] - 140s 8s/step - loss: 0.9186 - accuracy: 0.6530 - val_loss: 1.1353 - val_accuracy: 0.6259\n",
      "Epoch 599/1000\n",
      "18/18 [==============================] - 160s 9s/step - loss: 1.0116 - accuracy: 0.6353 - val_loss: 1.1957 - val_accuracy: 0.6259\n",
      "Epoch 600/1000\n",
      "18/18 [==============================] - 146s 8s/step - loss: 1.0850 - accuracy: 0.5786 - val_loss: 1.1027 - val_accuracy: 0.6475\n",
      "Epoch 601/1000\n",
      "18/18 [==============================] - 151s 8s/step - loss: 1.0208 - accuracy: 0.6224 - val_loss: 1.3059 - val_accuracy: 0.6475\n",
      "Epoch 602/1000\n",
      "18/18 [==============================] - 149s 8s/step - loss: 0.8762 - accuracy: 0.6629 - val_loss: 1.1149 - val_accuracy: 0.6547\n",
      "Epoch 603/1000\n",
      "18/18 [==============================] - 139s 8s/step - loss: 1.1051 - accuracy: 0.5492 - val_loss: 1.1260 - val_accuracy: 0.6259\n",
      "Epoch 604/1000\n",
      "18/18 [==============================] - 142s 8s/step - loss: 1.0906 - accuracy: 0.6247 - val_loss: 1.1304 - val_accuracy: 0.6547\n",
      "Epoch 605/1000\n",
      "18/18 [==============================] - 159s 9s/step - loss: 1.0105 - accuracy: 0.6161 - val_loss: 1.0718 - val_accuracy: 0.6691\n",
      "Epoch 606/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.9495 - accuracy: 0.6323 - val_loss: 1.1114 - val_accuracy: 0.6331\n",
      "Epoch 607/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.1296 - accuracy: 0.6410 - val_loss: 1.1252 - val_accuracy: 0.6475\n",
      "Epoch 608/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 0.9660 - accuracy: 0.6017 - val_loss: 1.1001 - val_accuracy: 0.6259\n",
      "Epoch 609/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.1632 - accuracy: 0.5817 - val_loss: 1.1184 - val_accuracy: 0.6115\n",
      "Epoch 610/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9585 - accuracy: 0.6198 - val_loss: 1.1089 - val_accuracy: 0.6547\n",
      "Epoch 611/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0045 - accuracy: 0.6407 - val_loss: 1.1406 - val_accuracy: 0.6115\n",
      "Epoch 612/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.9362 - accuracy: 0.6196 - val_loss: 1.0718 - val_accuracy: 0.6978\n",
      "Epoch 613/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9378 - accuracy: 0.6596 - val_loss: 1.2285 - val_accuracy: 0.6619\n",
      "Epoch 614/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9610 - accuracy: 0.6545 - val_loss: 1.1005 - val_accuracy: 0.6475\n",
      "Epoch 615/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.1128 - accuracy: 0.5524 - val_loss: 1.1059 - val_accuracy: 0.6331\n",
      "Epoch 616/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.1735 - accuracy: 0.5689 - val_loss: 1.0987 - val_accuracy: 0.6403\n",
      "Epoch 617/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9996 - accuracy: 0.6562 - val_loss: 1.1781 - val_accuracy: 0.6259\n",
      "Epoch 618/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9199 - accuracy: 0.6714 - val_loss: 1.0593 - val_accuracy: 0.6403\n",
      "Epoch 619/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0734 - accuracy: 0.6271 - val_loss: 1.1462 - val_accuracy: 0.6906\n",
      "Epoch 620/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9672 - accuracy: 0.5862 - val_loss: 1.1022 - val_accuracy: 0.6835\n",
      "Epoch 621/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9437 - accuracy: 0.6532 - val_loss: 1.0321 - val_accuracy: 0.6691\n",
      "Epoch 622/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9765 - accuracy: 0.6442 - val_loss: 1.0815 - val_accuracy: 0.6835\n",
      "Epoch 623/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9527 - accuracy: 0.6440 - val_loss: 1.0663 - val_accuracy: 0.6619\n",
      "Epoch 624/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.1344 - accuracy: 0.5802 - val_loss: 1.0231 - val_accuracy: 0.6763\n",
      "Epoch 625/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9117 - accuracy: 0.6752 - val_loss: 1.0365 - val_accuracy: 0.6619\n",
      "Epoch 626/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0328 - accuracy: 0.6251 - val_loss: 1.2084 - val_accuracy: 0.6547\n",
      "Epoch 627/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.8563 - accuracy: 0.6754 - val_loss: 1.0981 - val_accuracy: 0.6331\n",
      "Epoch 628/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.0155 - accuracy: 0.5699 - val_loss: 1.1564 - val_accuracy: 0.6403\n",
      "Epoch 629/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.0034 - accuracy: 0.6103 - val_loss: 1.1233 - val_accuracy: 0.6619\n",
      "Epoch 630/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9399 - accuracy: 0.6078 - val_loss: 1.2341 - val_accuracy: 0.6115\n",
      "Epoch 631/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9383 - accuracy: 0.6637 - val_loss: 1.2202 - val_accuracy: 0.6259\n",
      "Epoch 632/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.8892 - accuracy: 0.6746 - val_loss: 1.1339 - val_accuracy: 0.6115\n",
      "Epoch 633/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9698 - accuracy: 0.6056 - val_loss: 1.2660 - val_accuracy: 0.6475\n",
      "Epoch 634/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0387 - accuracy: 0.6196 - val_loss: 1.2374 - val_accuracy: 0.6547\n",
      "Epoch 635/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0814 - accuracy: 0.6002 - val_loss: 1.1640 - val_accuracy: 0.6259\n",
      "Epoch 636/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.0571 - accuracy: 0.5977 - val_loss: 1.3763 - val_accuracy: 0.6403\n",
      "Epoch 637/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0897 - accuracy: 0.5537 - val_loss: 1.1000 - val_accuracy: 0.6115\n",
      "Epoch 638/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9306 - accuracy: 0.6469 - val_loss: 1.0879 - val_accuracy: 0.6259\n",
      "Epoch 639/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9603 - accuracy: 0.6873 - val_loss: 1.1374 - val_accuracy: 0.6331\n",
      "Epoch 640/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9353 - accuracy: 0.6607 - val_loss: 1.1304 - val_accuracy: 0.6331\n",
      "Epoch 641/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8478 - accuracy: 0.6948 - val_loss: 1.1415 - val_accuracy: 0.6187\n",
      "Epoch 642/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.0277 - accuracy: 0.5946 - val_loss: 1.3105 - val_accuracy: 0.6403\n",
      "Epoch 643/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9239 - accuracy: 0.6389 - val_loss: 1.2064 - val_accuracy: 0.6403\n",
      "Epoch 644/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 1.0017 - accuracy: 0.6153 - val_loss: 1.5885 - val_accuracy: 0.6475\n",
      "Epoch 645/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0661 - accuracy: 0.5927 - val_loss: 1.2601 - val_accuracy: 0.6619\n",
      "Epoch 646/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.1119 - accuracy: 0.5879 - val_loss: 1.5885 - val_accuracy: 0.6475\n",
      "Epoch 647/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9663 - accuracy: 0.6128 - val_loss: 1.2569 - val_accuracy: 0.6475\n",
      "Epoch 648/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9509 - accuracy: 0.6429 - val_loss: 1.1739 - val_accuracy: 0.6331\n",
      "Epoch 649/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 1.0720 - accuracy: 0.5978 - val_loss: 1.1541 - val_accuracy: 0.6691\n",
      "Epoch 650/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 1.0204 - accuracy: 0.6270 - val_loss: 1.1220 - val_accuracy: 0.6763\n",
      "Epoch 651/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0579 - accuracy: 0.5649 - val_loss: 1.1075 - val_accuracy: 0.6691\n",
      "Epoch 652/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9539 - accuracy: 0.6603 - val_loss: 1.1667 - val_accuracy: 0.6547\n",
      "Epoch 653/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.8123 - accuracy: 0.6335 - val_loss: 1.0741 - val_accuracy: 0.6259\n",
      "Epoch 654/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9157 - accuracy: 0.6544 - val_loss: 1.0856 - val_accuracy: 0.6619\n",
      "Epoch 655/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9756 - accuracy: 0.6419 - val_loss: 1.1277 - val_accuracy: 0.6187\n",
      "Epoch 656/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9181 - accuracy: 0.6673 - val_loss: 1.1415 - val_accuracy: 0.6187\n",
      "Epoch 657/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8673 - accuracy: 0.6170 - val_loss: 1.1640 - val_accuracy: 0.6187\n",
      "Epoch 658/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 0.9637 - accuracy: 0.6299 - val_loss: 1.2101 - val_accuracy: 0.6331\n",
      "Epoch 659/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 0.9392 - accuracy: 0.6045 - val_loss: 1.2596 - val_accuracy: 0.6547\n",
      "Epoch 660/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9383 - accuracy: 0.6594 - val_loss: 1.1966 - val_accuracy: 0.6187\n",
      "Epoch 661/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0379 - accuracy: 0.6178 - val_loss: 1.1606 - val_accuracy: 0.6187\n",
      "Epoch 662/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0192 - accuracy: 0.5976 - val_loss: 1.1663 - val_accuracy: 0.6331\n",
      "Epoch 663/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.8867 - accuracy: 0.6728 - val_loss: 1.1242 - val_accuracy: 0.5899\n",
      "Epoch 664/1000\n",
      "18/18 [==============================] - 99s 5s/step - loss: 0.8818 - accuracy: 0.6830 - val_loss: 1.1049 - val_accuracy: 0.6331\n",
      "Epoch 665/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 1.0788 - accuracy: 0.5782 - val_loss: 1.0745 - val_accuracy: 0.6187\n",
      "Epoch 666/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9628 - accuracy: 0.6522 - val_loss: 1.1575 - val_accuracy: 0.6547\n",
      "Epoch 667/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9182 - accuracy: 0.5710 - val_loss: 1.2068 - val_accuracy: 0.6619\n",
      "Epoch 668/1000\n",
      "18/18 [==============================] - 99s 6s/step - loss: 0.9698 - accuracy: 0.6076 - val_loss: 1.3089 - val_accuracy: 0.6763\n",
      "Epoch 669/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.9054 - accuracy: 0.6342 - val_loss: 1.1848 - val_accuracy: 0.6619\n",
      "Epoch 670/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9606 - accuracy: 0.6560 - val_loss: 1.1219 - val_accuracy: 0.6763\n",
      "Epoch 671/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8409 - accuracy: 0.6768 - val_loss: 1.1125 - val_accuracy: 0.6835\n",
      "Epoch 672/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9972 - accuracy: 0.6289 - val_loss: 1.0533 - val_accuracy: 0.6619\n",
      "Epoch 673/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.8984 - accuracy: 0.6710 - val_loss: 1.2344 - val_accuracy: 0.6187\n",
      "Epoch 674/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8798 - accuracy: 0.6632 - val_loss: 1.2487 - val_accuracy: 0.6187\n",
      "Epoch 675/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.9420 - accuracy: 0.6288 - val_loss: 1.3641 - val_accuracy: 0.6763\n",
      "Epoch 676/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9156 - accuracy: 0.6807 - val_loss: 1.1808 - val_accuracy: 0.6475\n",
      "Epoch 677/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 1.1078 - accuracy: 0.5544 - val_loss: 1.1743 - val_accuracy: 0.6619\n",
      "Epoch 678/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9576 - accuracy: 0.6367 - val_loss: 1.1938 - val_accuracy: 0.6763\n",
      "Epoch 679/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 103s 6s/step - loss: 0.9073 - accuracy: 0.6328 - val_loss: 1.1871 - val_accuracy: 0.6043\n",
      "Epoch 680/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.9514 - accuracy: 0.5965 - val_loss: 1.3286 - val_accuracy: 0.6475\n",
      "Epoch 681/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0236 - accuracy: 0.6343 - val_loss: 1.1285 - val_accuracy: 0.6403\n",
      "Epoch 682/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0029 - accuracy: 0.6059 - val_loss: 1.1896 - val_accuracy: 0.6403\n",
      "Epoch 683/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.1332 - accuracy: 0.6416 - val_loss: 1.0746 - val_accuracy: 0.6475\n",
      "Epoch 684/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0096 - accuracy: 0.6458 - val_loss: 1.0752 - val_accuracy: 0.6475\n",
      "Epoch 685/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9823 - accuracy: 0.6296 - val_loss: 1.1385 - val_accuracy: 0.6475\n",
      "Epoch 686/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.9143 - accuracy: 0.6610 - val_loss: 1.0795 - val_accuracy: 0.6403\n",
      "Epoch 687/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0513 - accuracy: 0.5677 - val_loss: 1.1627 - val_accuracy: 0.6331\n",
      "Epoch 688/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0246 - accuracy: 0.6374 - val_loss: 1.1255 - val_accuracy: 0.6331\n",
      "Epoch 689/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9345 - accuracy: 0.6323 - val_loss: 1.1532 - val_accuracy: 0.6187\n",
      "Epoch 690/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8232 - accuracy: 0.6885 - val_loss: 1.4280 - val_accuracy: 0.6331\n",
      "Epoch 691/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0095 - accuracy: 0.6499 - val_loss: 1.2133 - val_accuracy: 0.6259\n",
      "Epoch 692/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9720 - accuracy: 0.5933 - val_loss: 1.2559 - val_accuracy: 0.5971\n",
      "Epoch 693/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8495 - accuracy: 0.6684 - val_loss: 1.1594 - val_accuracy: 0.6187\n",
      "Epoch 694/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8294 - accuracy: 0.7105 - val_loss: 1.1673 - val_accuracy: 0.6115\n",
      "Epoch 695/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8890 - accuracy: 0.7031 - val_loss: 1.1324 - val_accuracy: 0.6475\n",
      "Epoch 696/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.7803 - accuracy: 0.7103 - val_loss: 1.4620 - val_accuracy: 0.6187\n",
      "Epoch 697/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 0.9706 - accuracy: 0.5872 - val_loss: 1.2039 - val_accuracy: 0.6547\n",
      "Epoch 698/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0748 - accuracy: 0.6024 - val_loss: 1.2404 - val_accuracy: 0.6619\n",
      "Epoch 699/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9856 - accuracy: 0.5807 - val_loss: 1.1192 - val_accuracy: 0.6403\n",
      "Epoch 700/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 1.0264 - accuracy: 0.6351 - val_loss: 1.1093 - val_accuracy: 0.6763\n",
      "Epoch 701/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0103 - accuracy: 0.6084 - val_loss: 1.1930 - val_accuracy: 0.6331\n",
      "Epoch 702/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9418 - accuracy: 0.6843 - val_loss: 1.2792 - val_accuracy: 0.6691\n",
      "Epoch 703/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 1.0533 - accuracy: 0.6328 - val_loss: 1.1321 - val_accuracy: 0.6331\n",
      "Epoch 704/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9505 - accuracy: 0.6432 - val_loss: 1.1043 - val_accuracy: 0.6475\n",
      "Epoch 705/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0005 - accuracy: 0.6848 - val_loss: 1.1888 - val_accuracy: 0.6403\n",
      "Epoch 706/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.8391 - accuracy: 0.6832 - val_loss: 1.1157 - val_accuracy: 0.6475\n",
      "Epoch 707/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0845 - accuracy: 0.6338 - val_loss: 1.0628 - val_accuracy: 0.6835\n",
      "Epoch 708/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.9768 - accuracy: 0.6075 - val_loss: 1.0696 - val_accuracy: 0.6547\n",
      "Epoch 709/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.0494 - accuracy: 0.5826 - val_loss: 1.0964 - val_accuracy: 0.6115\n",
      "Epoch 710/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.9710 - accuracy: 0.6904 - val_loss: 1.3113 - val_accuracy: 0.6403\n",
      "Epoch 711/1000\n",
      "18/18 [==============================] - 124s 7s/step - loss: 1.0028 - accuracy: 0.6024 - val_loss: 1.1591 - val_accuracy: 0.5899\n",
      "Epoch 712/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8639 - accuracy: 0.6672 - val_loss: 1.1377 - val_accuracy: 0.6547\n",
      "Epoch 713/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0015 - accuracy: 0.6302 - val_loss: 1.1998 - val_accuracy: 0.6403\n",
      "Epoch 714/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 1.0178 - accuracy: 0.6360 - val_loss: 1.1311 - val_accuracy: 0.6403\n",
      "Epoch 715/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9855 - accuracy: 0.6562 - val_loss: 1.2335 - val_accuracy: 0.6403\n",
      "Epoch 716/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.9419 - accuracy: 0.6375 - val_loss: 1.1237 - val_accuracy: 0.6331\n",
      "Epoch 717/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0228 - accuracy: 0.5587 - val_loss: 1.2160 - val_accuracy: 0.6331\n",
      "Epoch 718/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9129 - accuracy: 0.6395 - val_loss: 1.2353 - val_accuracy: 0.6691\n",
      "Epoch 719/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.8693 - accuracy: 0.6638 - val_loss: 1.1659 - val_accuracy: 0.6547\n",
      "Epoch 720/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9925 - accuracy: 0.6623 - val_loss: 1.4745 - val_accuracy: 0.6403\n",
      "Epoch 721/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8706 - accuracy: 0.6358 - val_loss: 1.1432 - val_accuracy: 0.6619\n",
      "Epoch 722/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 1.0262 - accuracy: 0.5864 - val_loss: 1.1696 - val_accuracy: 0.6619\n",
      "Epoch 723/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9375 - accuracy: 0.6610 - val_loss: 1.1177 - val_accuracy: 0.6835\n",
      "Epoch 724/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9230 - accuracy: 0.6559 - val_loss: 1.2093 - val_accuracy: 0.6763\n",
      "Epoch 725/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.9129 - accuracy: 0.6211 - val_loss: 1.2055 - val_accuracy: 0.6763\n",
      "Epoch 726/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8623 - accuracy: 0.7111 - val_loss: 1.2222 - val_accuracy: 0.6835\n",
      "Epoch 727/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8647 - accuracy: 0.6735 - val_loss: 1.1606 - val_accuracy: 0.6906\n",
      "Epoch 728/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8896 - accuracy: 0.6807 - val_loss: 1.1118 - val_accuracy: 0.6763\n",
      "Epoch 729/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8729 - accuracy: 0.6991 - val_loss: 1.1983 - val_accuracy: 0.6906\n",
      "Epoch 730/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9980 - accuracy: 0.6316 - val_loss: 1.0980 - val_accuracy: 0.6691\n",
      "Epoch 731/1000\n",
      "18/18 [==============================] - 124s 7s/step - loss: 1.0005 - accuracy: 0.5952 - val_loss: 1.2494 - val_accuracy: 0.6691\n",
      "Epoch 732/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.7801 - accuracy: 0.7226 - val_loss: 1.1676 - val_accuracy: 0.6331\n",
      "Epoch 733/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0607 - accuracy: 0.6260 - val_loss: 1.2086 - val_accuracy: 0.6475\n",
      "Epoch 734/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9401 - accuracy: 0.6310 - val_loss: 1.1807 - val_accuracy: 0.6331\n",
      "Epoch 735/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.9468 - accuracy: 0.7116 - val_loss: 1.1718 - val_accuracy: 0.6763\n",
      "Epoch 736/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.9916 - accuracy: 0.6363 - val_loss: 1.1219 - val_accuracy: 0.6835\n",
      "Epoch 737/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8528 - accuracy: 0.6791 - val_loss: 1.0989 - val_accuracy: 0.6691\n",
      "Epoch 738/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8413 - accuracy: 0.6898 - val_loss: 1.1098 - val_accuracy: 0.6906\n",
      "Epoch 739/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8857 - accuracy: 0.6515 - val_loss: 1.1249 - val_accuracy: 0.6259\n",
      "Epoch 740/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8653 - accuracy: 0.6730 - val_loss: 1.2904 - val_accuracy: 0.6331\n",
      "Epoch 741/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0385 - accuracy: 0.6271 - val_loss: 1.1009 - val_accuracy: 0.6403\n",
      "Epoch 742/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8961 - accuracy: 0.6537 - val_loss: 1.0564 - val_accuracy: 0.6619\n",
      "Epoch 743/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.9781 - accuracy: 0.6010 - val_loss: 1.1471 - val_accuracy: 0.6691\n",
      "Epoch 744/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9143 - accuracy: 0.6635 - val_loss: 1.1749 - val_accuracy: 0.6547\n",
      "Epoch 745/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 1.0646 - accuracy: 0.6159 - val_loss: 1.0896 - val_accuracy: 0.6475\n",
      "Epoch 746/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9529 - accuracy: 0.6393 - val_loss: 1.0414 - val_accuracy: 0.6475\n",
      "Epoch 747/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8789 - accuracy: 0.6719 - val_loss: 1.0245 - val_accuracy: 0.6403\n",
      "Epoch 748/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.8148 - accuracy: 0.6571 - val_loss: 1.1762 - val_accuracy: 0.6403\n",
      "Epoch 749/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0442 - accuracy: 0.6450 - val_loss: 1.1502 - val_accuracy: 0.6331\n",
      "Epoch 750/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9076 - accuracy: 0.6683 - val_loss: 1.0872 - val_accuracy: 0.6475\n",
      "Epoch 751/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.8549 - accuracy: 0.6516 - val_loss: 1.1759 - val_accuracy: 0.6403\n",
      "Epoch 752/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9338 - accuracy: 0.6806 - val_loss: 1.1303 - val_accuracy: 0.6187\n",
      "Epoch 753/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.8878 - accuracy: 0.6987 - val_loss: 1.1363 - val_accuracy: 0.6115\n",
      "Epoch 754/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0423 - accuracy: 0.5588 - val_loss: 1.2087 - val_accuracy: 0.6043\n",
      "Epoch 755/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0444 - accuracy: 0.6143 - val_loss: 1.2806 - val_accuracy: 0.6187\n",
      "Epoch 756/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0298 - accuracy: 0.6311 - val_loss: 1.1504 - val_accuracy: 0.5827\n",
      "Epoch 757/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0504 - accuracy: 0.6383 - val_loss: 1.0993 - val_accuracy: 0.6763\n",
      "Epoch 758/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8093 - accuracy: 0.7017 - val_loss: 1.1421 - val_accuracy: 0.6403\n",
      "Epoch 759/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9441 - accuracy: 0.6510 - val_loss: 1.3928 - val_accuracy: 0.6187\n",
      "Epoch 760/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9303 - accuracy: 0.6871 - val_loss: 1.1469 - val_accuracy: 0.6331\n",
      "Epoch 761/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8691 - accuracy: 0.6524 - val_loss: 1.1285 - val_accuracy: 0.6259\n",
      "Epoch 762/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7710 - accuracy: 0.6952 - val_loss: 1.1041 - val_accuracy: 0.6043\n",
      "Epoch 763/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 1.0503 - accuracy: 0.6383 - val_loss: 1.2094 - val_accuracy: 0.6763\n",
      "Epoch 764/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9030 - accuracy: 0.6083 - val_loss: 1.1370 - val_accuracy: 0.6403\n",
      "Epoch 765/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 1.0336 - accuracy: 0.6430 - val_loss: 1.2405 - val_accuracy: 0.6259\n",
      "Epoch 766/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8389 - accuracy: 0.7058 - val_loss: 1.2412 - val_accuracy: 0.6403\n",
      "Epoch 767/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.9735 - accuracy: 0.6597 - val_loss: 1.4437 - val_accuracy: 0.6475\n",
      "Epoch 768/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8824 - accuracy: 0.6853 - val_loss: 1.2163 - val_accuracy: 0.6978\n",
      "Epoch 769/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9442 - accuracy: 0.6618 - val_loss: 1.2702 - val_accuracy: 0.6547\n",
      "Epoch 770/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9758 - accuracy: 0.6420 - val_loss: 1.1446 - val_accuracy: 0.5971\n",
      "Epoch 771/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8490 - accuracy: 0.6793 - val_loss: 1.2129 - val_accuracy: 0.6259\n",
      "Epoch 772/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8721 - accuracy: 0.7004 - val_loss: 1.3503 - val_accuracy: 0.6043\n",
      "Epoch 773/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0034 - accuracy: 0.5686 - val_loss: 1.1751 - val_accuracy: 0.5971\n",
      "Epoch 774/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9814 - accuracy: 0.6215 - val_loss: 1.2392 - val_accuracy: 0.6547\n",
      "Epoch 775/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9408 - accuracy: 0.6371 - val_loss: 1.1274 - val_accuracy: 0.6043\n",
      "Epoch 776/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.8562 - accuracy: 0.7003 - val_loss: 1.1933 - val_accuracy: 0.6115\n",
      "Epoch 777/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8503 - accuracy: 0.6431 - val_loss: 1.3408 - val_accuracy: 0.6475\n",
      "Epoch 778/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9463 - accuracy: 0.6438 - val_loss: 1.1262 - val_accuracy: 0.6691\n",
      "Epoch 779/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9484 - accuracy: 0.6624 - val_loss: 1.0943 - val_accuracy: 0.6835\n",
      "Epoch 780/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8802 - accuracy: 0.6457 - val_loss: 1.0918 - val_accuracy: 0.7050\n",
      "Epoch 781/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9280 - accuracy: 0.6710 - val_loss: 1.1803 - val_accuracy: 0.6331\n",
      "Epoch 782/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9476 - accuracy: 0.6490 - val_loss: 1.1687 - val_accuracy: 0.6763\n",
      "Epoch 783/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8804 - accuracy: 0.6686 - val_loss: 1.0981 - val_accuracy: 0.6619\n",
      "Epoch 784/1000\n",
      "18/18 [==============================] - 100s 6s/step - loss: 0.9375 - accuracy: 0.6321 - val_loss: 1.2076 - val_accuracy: 0.6475\n",
      "Epoch 785/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9380 - accuracy: 0.6517 - val_loss: 1.1851 - val_accuracy: 0.6475\n",
      "Epoch 786/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8354 - accuracy: 0.6697 - val_loss: 1.0994 - val_accuracy: 0.6691\n",
      "Epoch 787/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9478 - accuracy: 0.6712 - val_loss: 1.1234 - val_accuracy: 0.6475\n",
      "Epoch 788/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8775 - accuracy: 0.6904 - val_loss: 1.1834 - val_accuracy: 0.6403\n",
      "Epoch 789/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9574 - accuracy: 0.6548 - val_loss: 1.1286 - val_accuracy: 0.6403\n",
      "Epoch 790/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8042 - accuracy: 0.7150 - val_loss: 1.0705 - val_accuracy: 0.6835\n",
      "Epoch 791/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 100s 6s/step - loss: 0.8764 - accuracy: 0.6938 - val_loss: 1.0980 - val_accuracy: 0.6403\n",
      "Epoch 792/1000\n",
      "18/18 [==============================] - 127s 7s/step - loss: 0.8156 - accuracy: 0.6365 - val_loss: 1.1148 - val_accuracy: 0.6187\n",
      "Epoch 793/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 0.9090 - accuracy: 0.6717 - val_loss: 1.1034 - val_accuracy: 0.6547\n",
      "Epoch 794/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8088 - accuracy: 0.7064 - val_loss: 1.1300 - val_accuracy: 0.6763\n",
      "Epoch 795/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9119 - accuracy: 0.6291 - val_loss: 1.1573 - val_accuracy: 0.6835\n",
      "Epoch 796/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9121 - accuracy: 0.7376 - val_loss: 1.2032 - val_accuracy: 0.6619\n",
      "Epoch 797/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9274 - accuracy: 0.6589 - val_loss: 1.2126 - val_accuracy: 0.6619\n",
      "Epoch 798/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9691 - accuracy: 0.6477 - val_loss: 1.2195 - val_accuracy: 0.6547\n",
      "Epoch 799/1000\n",
      "18/18 [==============================] - 121s 7s/step - loss: 0.9794 - accuracy: 0.6164 - val_loss: 1.1203 - val_accuracy: 0.6259\n",
      "Epoch 800/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 0.9131 - accuracy: 0.6317 - val_loss: 1.0708 - val_accuracy: 0.6403\n",
      "Epoch 801/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.9550 - accuracy: 0.6555 - val_loss: 1.2215 - val_accuracy: 0.6475\n",
      "Epoch 802/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.8776 - accuracy: 0.6745 - val_loss: 1.1642 - val_accuracy: 0.6187\n",
      "Epoch 803/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.8500 - accuracy: 0.7015 - val_loss: 1.2073 - val_accuracy: 0.6403\n",
      "Epoch 804/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 0.8316 - accuracy: 0.6793 - val_loss: 1.1347 - val_accuracy: 0.6691\n",
      "Epoch 805/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 1.0116 - accuracy: 0.6439 - val_loss: 1.0741 - val_accuracy: 0.6619\n",
      "Epoch 806/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 0.9078 - accuracy: 0.6619 - val_loss: 1.1913 - val_accuracy: 0.6763\n",
      "Epoch 807/1000\n",
      "18/18 [==============================] - 123s 7s/step - loss: 0.8274 - accuracy: 0.7321 - val_loss: 1.1569 - val_accuracy: 0.6619\n",
      "Epoch 808/1000\n",
      "18/18 [==============================] - 137s 8s/step - loss: 0.8935 - accuracy: 0.6961 - val_loss: 1.1418 - val_accuracy: 0.6115\n",
      "Epoch 809/1000\n",
      "18/18 [==============================] - 144s 8s/step - loss: 0.8531 - accuracy: 0.6633 - val_loss: 1.2759 - val_accuracy: 0.5827\n",
      "Epoch 810/1000\n",
      "18/18 [==============================] - 135s 7s/step - loss: 0.9073 - accuracy: 0.6556 - val_loss: 1.1886 - val_accuracy: 0.6547\n",
      "Epoch 811/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 0.9045 - accuracy: 0.6684 - val_loss: 1.1787 - val_accuracy: 0.6619\n",
      "Epoch 812/1000\n",
      "18/18 [==============================] - 133s 7s/step - loss: 0.8511 - accuracy: 0.6936 - val_loss: 1.1790 - val_accuracy: 0.6763\n",
      "Epoch 813/1000\n",
      "18/18 [==============================] - 139s 8s/step - loss: 0.9526 - accuracy: 0.6147 - val_loss: 1.1772 - val_accuracy: 0.6978\n",
      "Epoch 814/1000\n",
      "18/18 [==============================] - 142s 8s/step - loss: 0.8863 - accuracy: 0.6250 - val_loss: 1.1729 - val_accuracy: 0.6475\n",
      "Epoch 815/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 1.0774 - accuracy: 0.6223 - val_loss: 1.0781 - val_accuracy: 0.6763\n",
      "Epoch 816/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8238 - accuracy: 0.6878 - val_loss: 1.1388 - val_accuracy: 0.6978\n",
      "Epoch 817/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.9267 - accuracy: 0.6409 - val_loss: 1.0806 - val_accuracy: 0.6691\n",
      "Epoch 818/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7773 - accuracy: 0.7309 - val_loss: 1.1193 - val_accuracy: 0.6763\n",
      "Epoch 819/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.9362 - accuracy: 0.6643 - val_loss: 1.0538 - val_accuracy: 0.6619\n",
      "Epoch 820/1000\n",
      "18/18 [==============================] - 116s 6s/step - loss: 0.9458 - accuracy: 0.6721 - val_loss: 1.0416 - val_accuracy: 0.6906\n",
      "Epoch 821/1000\n",
      "18/18 [==============================] - 117s 6s/step - loss: 0.9401 - accuracy: 0.6455 - val_loss: 0.9985 - val_accuracy: 0.7050\n",
      "Epoch 822/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8769 - accuracy: 0.6173 - val_loss: 1.0443 - val_accuracy: 0.6619\n",
      "Epoch 823/1000\n",
      "18/18 [==============================] - 120s 7s/step - loss: 0.9735 - accuracy: 0.6184 - val_loss: 1.0333 - val_accuracy: 0.6906\n",
      "Epoch 824/1000\n",
      "18/18 [==============================] - 120s 7s/step - loss: 0.8626 - accuracy: 0.6599 - val_loss: 1.0843 - val_accuracy: 0.6403\n",
      "Epoch 825/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 0.7805 - accuracy: 0.7083 - val_loss: 1.0818 - val_accuracy: 0.6763\n",
      "Epoch 826/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.8760 - accuracy: 0.6660 - val_loss: 1.0219 - val_accuracy: 0.6547\n",
      "Epoch 827/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7621 - accuracy: 0.7343 - val_loss: 1.1282 - val_accuracy: 0.6835\n",
      "Epoch 828/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7711 - accuracy: 0.7039 - val_loss: 1.1313 - val_accuracy: 0.6835\n",
      "Epoch 829/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.7871 - accuracy: 0.6883 - val_loss: 1.0402 - val_accuracy: 0.6763\n",
      "Epoch 830/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8911 - accuracy: 0.6724 - val_loss: 1.0426 - val_accuracy: 0.6691\n",
      "Epoch 831/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7951 - accuracy: 0.7107 - val_loss: 1.0429 - val_accuracy: 0.6835\n",
      "Epoch 832/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7634 - accuracy: 0.6678 - val_loss: 1.1368 - val_accuracy: 0.6691\n",
      "Epoch 833/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7819 - accuracy: 0.6783 - val_loss: 1.1758 - val_accuracy: 0.6619\n",
      "Epoch 834/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8022 - accuracy: 0.6998 - val_loss: 1.1169 - val_accuracy: 0.6835\n",
      "Epoch 835/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.7610 - accuracy: 0.6903 - val_loss: 1.2303 - val_accuracy: 0.7050\n",
      "Epoch 836/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9010 - accuracy: 0.6625 - val_loss: 1.2562 - val_accuracy: 0.6835\n",
      "Epoch 837/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.7895 - accuracy: 0.6829 - val_loss: 1.0537 - val_accuracy: 0.6763\n",
      "Epoch 838/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7943 - accuracy: 0.6885 - val_loss: 1.1495 - val_accuracy: 0.6475\n",
      "Epoch 839/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9584 - accuracy: 0.6381 - val_loss: 1.1044 - val_accuracy: 0.6835\n",
      "Epoch 840/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8452 - accuracy: 0.6970 - val_loss: 1.1295 - val_accuracy: 0.6619\n",
      "Epoch 841/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.7717 - accuracy: 0.6742 - val_loss: 1.0423 - val_accuracy: 0.6906\n",
      "Epoch 842/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8390 - accuracy: 0.6848 - val_loss: 1.0471 - val_accuracy: 0.7050\n",
      "Epoch 843/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.9927 - accuracy: 0.6230 - val_loss: 1.0650 - val_accuracy: 0.6978\n",
      "Epoch 844/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.7851 - accuracy: 0.6916 - val_loss: 1.1266 - val_accuracy: 0.6547\n",
      "Epoch 845/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7706 - accuracy: 0.7433 - val_loss: 1.1280 - val_accuracy: 0.6187\n",
      "Epoch 846/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.7772 - accuracy: 0.7194 - val_loss: 1.1288 - val_accuracy: 0.5971\n",
      "Epoch 847/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.8884 - accuracy: 0.6789 - val_loss: 1.0494 - val_accuracy: 0.6619\n",
      "Epoch 848/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8051 - accuracy: 0.6849 - val_loss: 1.2356 - val_accuracy: 0.6619\n",
      "Epoch 849/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8263 - accuracy: 0.7183 - val_loss: 1.1224 - val_accuracy: 0.6187\n",
      "Epoch 850/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7155 - accuracy: 0.7247 - val_loss: 1.3273 - val_accuracy: 0.6403\n",
      "Epoch 851/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8141 - accuracy: 0.7200 - val_loss: 1.0934 - val_accuracy: 0.5899\n",
      "Epoch 852/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8801 - accuracy: 0.6233 - val_loss: 1.3251 - val_accuracy: 0.6259\n",
      "Epoch 853/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9727 - accuracy: 0.6743 - val_loss: 1.2153 - val_accuracy: 0.6619\n",
      "Epoch 854/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8295 - accuracy: 0.6738 - val_loss: 1.1030 - val_accuracy: 0.6691\n",
      "Epoch 855/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8204 - accuracy: 0.7125 - val_loss: 1.0897 - val_accuracy: 0.6475\n",
      "Epoch 856/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8074 - accuracy: 0.6884 - val_loss: 1.1528 - val_accuracy: 0.6547\n",
      "Epoch 857/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7945 - accuracy: 0.6537 - val_loss: 1.1237 - val_accuracy: 0.6403\n",
      "Epoch 858/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.9128 - accuracy: 0.6972 - val_loss: 1.1343 - val_accuracy: 0.6475\n",
      "Epoch 859/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7470 - accuracy: 0.7443 - val_loss: 1.1693 - val_accuracy: 0.6043\n",
      "Epoch 860/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7946 - accuracy: 0.6924 - val_loss: 1.0738 - val_accuracy: 0.6763\n",
      "Epoch 861/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8982 - accuracy: 0.6869 - val_loss: 1.0204 - val_accuracy: 0.6763\n",
      "Epoch 862/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8253 - accuracy: 0.7204 - val_loss: 1.0770 - val_accuracy: 0.6906\n",
      "Epoch 863/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.9015 - accuracy: 0.6599 - val_loss: 1.0604 - val_accuracy: 0.6691\n",
      "Epoch 864/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.9373 - accuracy: 0.6635 - val_loss: 1.1191 - val_accuracy: 0.6403\n",
      "Epoch 865/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9283 - accuracy: 0.6664 - val_loss: 1.1465 - val_accuracy: 0.6187\n",
      "Epoch 866/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8641 - accuracy: 0.6772 - val_loss: 1.2137 - val_accuracy: 0.6043\n",
      "Epoch 867/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.7851 - accuracy: 0.7165 - val_loss: 1.1670 - val_accuracy: 0.6259\n",
      "Epoch 868/1000\n",
      "18/18 [==============================] - 118s 7s/step - loss: 0.9239 - accuracy: 0.6130 - val_loss: 1.1698 - val_accuracy: 0.6475\n",
      "Epoch 869/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8475 - accuracy: 0.6780 - val_loss: 1.1850 - val_accuracy: 0.6331\n",
      "Epoch 870/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8466 - accuracy: 0.6722 - val_loss: 1.1410 - val_accuracy: 0.6403\n",
      "Epoch 871/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8243 - accuracy: 0.6957 - val_loss: 1.1470 - val_accuracy: 0.6619\n",
      "Epoch 872/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8818 - accuracy: 0.6654 - val_loss: 1.2690 - val_accuracy: 0.6331\n",
      "Epoch 873/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8755 - accuracy: 0.6492 - val_loss: 1.2045 - val_accuracy: 0.6259\n",
      "Epoch 874/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.6951 - accuracy: 0.7433 - val_loss: 1.2434 - val_accuracy: 0.5827\n",
      "Epoch 875/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.8662 - accuracy: 0.6605 - val_loss: 1.1278 - val_accuracy: 0.6691\n",
      "Epoch 876/1000\n",
      "18/18 [==============================] - 119s 7s/step - loss: 0.7936 - accuracy: 0.6893 - val_loss: 1.2169 - val_accuracy: 0.6619\n",
      "Epoch 877/1000\n",
      "18/18 [==============================] - 135s 8s/step - loss: 0.8660 - accuracy: 0.6977 - val_loss: 1.2303 - val_accuracy: 0.6475\n",
      "Epoch 878/1000\n",
      "18/18 [==============================] - 131s 7s/step - loss: 0.9523 - accuracy: 0.6408 - val_loss: 1.2300 - val_accuracy: 0.6259\n",
      "Epoch 879/1000\n",
      "18/18 [==============================] - 138s 7s/step - loss: 0.9644 - accuracy: 0.6323 - val_loss: 1.2022 - val_accuracy: 0.6691\n",
      "Epoch 880/1000\n",
      "18/18 [==============================] - 165s 9s/step - loss: 0.8601 - accuracy: 0.7239 - val_loss: 1.1621 - val_accuracy: 0.6259\n",
      "Epoch 881/1000\n",
      "18/18 [==============================] - 130s 7s/step - loss: 0.7742 - accuracy: 0.7181 - val_loss: 1.1521 - val_accuracy: 0.6547\n",
      "Epoch 882/1000\n",
      "18/18 [==============================] - 122s 7s/step - loss: 0.8051 - accuracy: 0.7396 - val_loss: 1.2851 - val_accuracy: 0.6547\n",
      "Epoch 883/1000\n",
      "18/18 [==============================] - 143s 8s/step - loss: 0.7299 - accuracy: 0.6897 - val_loss: 1.2560 - val_accuracy: 0.6187\n",
      "Epoch 884/1000\n",
      "18/18 [==============================] - 149s 8s/step - loss: 0.8769 - accuracy: 0.6675 - val_loss: 1.3158 - val_accuracy: 0.6619\n",
      "Epoch 885/1000\n",
      "18/18 [==============================] - 136s 8s/step - loss: 0.7862 - accuracy: 0.7378 - val_loss: 1.2034 - val_accuracy: 0.6331\n",
      "Epoch 886/1000\n",
      "18/18 [==============================] - 143s 8s/step - loss: 0.8929 - accuracy: 0.6437 - val_loss: 1.2534 - val_accuracy: 0.6475\n",
      "Epoch 887/1000\n",
      "18/18 [==============================] - 124s 7s/step - loss: 0.8137 - accuracy: 0.7041 - val_loss: 1.2652 - val_accuracy: 0.6403\n",
      "Epoch 888/1000\n",
      "18/18 [==============================] - 161s 9s/step - loss: 0.8648 - accuracy: 0.6971 - val_loss: 1.3713 - val_accuracy: 0.6547\n",
      "Epoch 889/1000\n",
      "18/18 [==============================] - 152s 8s/step - loss: 0.7743 - accuracy: 0.7276 - val_loss: 1.2932 - val_accuracy: 0.6259\n",
      "Epoch 890/1000\n",
      "18/18 [==============================] - 135s 7s/step - loss: 0.8353 - accuracy: 0.7550 - val_loss: 1.3159 - val_accuracy: 0.6619\n",
      "Epoch 891/1000\n",
      "18/18 [==============================] - 140s 8s/step - loss: 0.7758 - accuracy: 0.7117 - val_loss: 1.1555 - val_accuracy: 0.6619\n",
      "Epoch 892/1000\n",
      "18/18 [==============================] - 138s 8s/step - loss: 0.8254 - accuracy: 0.7026 - val_loss: 1.2106 - val_accuracy: 0.6691\n",
      "Epoch 893/1000\n",
      "18/18 [==============================] - 119s 6s/step - loss: 0.7871 - accuracy: 0.7062 - val_loss: 1.3024 - val_accuracy: 0.6403\n",
      "Epoch 894/1000\n",
      "18/18 [==============================] - 112s 6s/step - loss: 0.8052 - accuracy: 0.6953 - val_loss: 1.1591 - val_accuracy: 0.6619\n",
      "Epoch 895/1000\n",
      "18/18 [==============================] - 116s 7s/step - loss: 0.8894 - accuracy: 0.6598 - val_loss: 1.2164 - val_accuracy: 0.6475\n",
      "Epoch 896/1000\n",
      "18/18 [==============================] - 115s 6s/step - loss: 0.7531 - accuracy: 0.7347 - val_loss: 1.1909 - val_accuracy: 0.6115\n",
      "Epoch 897/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.8619 - accuracy: 0.6961 - val_loss: 1.2096 - val_accuracy: 0.6403\n",
      "Epoch 898/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.7120 - accuracy: 0.7408 - val_loss: 1.2058 - val_accuracy: 0.6547\n",
      "Epoch 899/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7190 - accuracy: 0.7378 - val_loss: 1.2238 - val_accuracy: 0.6403\n",
      "Epoch 900/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8001 - accuracy: 0.6904 - val_loss: 1.2620 - val_accuracy: 0.6547\n",
      "Epoch 901/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8577 - accuracy: 0.6557 - val_loss: 1.1809 - val_accuracy: 0.6259\n",
      "Epoch 902/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8564 - accuracy: 0.6575 - val_loss: 1.1428 - val_accuracy: 0.6978\n",
      "Epoch 903/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 109s 6s/step - loss: 0.9158 - accuracy: 0.6544 - val_loss: 1.1303 - val_accuracy: 0.6547\n",
      "Epoch 904/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8094 - accuracy: 0.6751 - val_loss: 1.2247 - val_accuracy: 0.6906\n",
      "Epoch 905/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7650 - accuracy: 0.7055 - val_loss: 1.3025 - val_accuracy: 0.6259\n",
      "Epoch 906/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8534 - accuracy: 0.7093 - val_loss: 1.1216 - val_accuracy: 0.6978\n",
      "Epoch 907/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8003 - accuracy: 0.7086 - val_loss: 1.0962 - val_accuracy: 0.6619\n",
      "Epoch 908/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8633 - accuracy: 0.6763 - val_loss: 1.1629 - val_accuracy: 0.6835\n",
      "Epoch 909/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.7568 - accuracy: 0.7064 - val_loss: 1.1563 - val_accuracy: 0.6835\n",
      "Epoch 910/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8427 - accuracy: 0.6640 - val_loss: 1.1556 - val_accuracy: 0.6691\n",
      "Epoch 911/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.7866 - accuracy: 0.7032 - val_loss: 1.0778 - val_accuracy: 0.6835\n",
      "Epoch 912/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.7442 - accuracy: 0.7278 - val_loss: 1.1613 - val_accuracy: 0.6619\n",
      "Epoch 913/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8903 - accuracy: 0.6793 - val_loss: 1.1856 - val_accuracy: 0.6547\n",
      "Epoch 914/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8237 - accuracy: 0.7165 - val_loss: 1.0069 - val_accuracy: 0.6763\n",
      "Epoch 915/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.8287 - accuracy: 0.6635 - val_loss: 1.0469 - val_accuracy: 0.6691\n",
      "Epoch 916/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8204 - accuracy: 0.6641 - val_loss: 1.1728 - val_accuracy: 0.6547\n",
      "Epoch 917/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.6600 - accuracy: 0.8051 - val_loss: 1.1553 - val_accuracy: 0.6619\n",
      "Epoch 918/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.9668 - accuracy: 0.6387 - val_loss: 1.1661 - val_accuracy: 0.6619\n",
      "Epoch 919/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8897 - accuracy: 0.6562 - val_loss: 1.1515 - val_accuracy: 0.6906\n",
      "Epoch 920/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8306 - accuracy: 0.6766 - val_loss: 1.1362 - val_accuracy: 0.6691\n",
      "Epoch 921/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7476 - accuracy: 0.7152 - val_loss: 1.1388 - val_accuracy: 0.7050\n",
      "Epoch 922/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8513 - accuracy: 0.7020 - val_loss: 1.2995 - val_accuracy: 0.6187\n",
      "Epoch 923/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9262 - accuracy: 0.6416 - val_loss: 1.2021 - val_accuracy: 0.6763\n",
      "Epoch 924/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8246 - accuracy: 0.6685 - val_loss: 1.1852 - val_accuracy: 0.6619\n",
      "Epoch 925/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8634 - accuracy: 0.7115 - val_loss: 1.1128 - val_accuracy: 0.6835\n",
      "Epoch 926/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.8162 - accuracy: 0.6923 - val_loss: 1.1989 - val_accuracy: 0.6331\n",
      "Epoch 927/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7091 - accuracy: 0.7170 - val_loss: 1.1727 - val_accuracy: 0.6835\n",
      "Epoch 928/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9384 - accuracy: 0.6354 - val_loss: 1.1218 - val_accuracy: 0.6763\n",
      "Epoch 929/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8551 - accuracy: 0.6951 - val_loss: 1.0800 - val_accuracy: 0.6835\n",
      "Epoch 930/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7329 - accuracy: 0.6881 - val_loss: 1.1817 - val_accuracy: 0.6403\n",
      "Epoch 931/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.6866 - accuracy: 0.7605 - val_loss: 1.2854 - val_accuracy: 0.6259\n",
      "Epoch 932/1000\n",
      "18/18 [==============================] - 109s 6s/step - loss: 0.8200 - accuracy: 0.7062 - val_loss: 1.3394 - val_accuracy: 0.6187\n",
      "Epoch 933/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8877 - accuracy: 0.6553 - val_loss: 1.1896 - val_accuracy: 0.6475\n",
      "Epoch 934/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.9019 - accuracy: 0.6614 - val_loss: 1.1111 - val_accuracy: 0.6619\n",
      "Epoch 935/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7297 - accuracy: 0.7276 - val_loss: 1.1508 - val_accuracy: 0.6619\n",
      "Epoch 936/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8295 - accuracy: 0.6626 - val_loss: 1.1870 - val_accuracy: 0.6619\n",
      "Epoch 937/1000\n",
      "18/18 [==============================] - 108s 6s/step - loss: 0.8559 - accuracy: 0.6823 - val_loss: 1.2771 - val_accuracy: 0.6835\n",
      "Epoch 938/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.7754 - accuracy: 0.7208 - val_loss: 1.2427 - val_accuracy: 0.6475\n",
      "Epoch 939/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8406 - accuracy: 0.6559 - val_loss: 1.2689 - val_accuracy: 0.6187\n",
      "Epoch 940/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9412 - accuracy: 0.6501 - val_loss: 1.2235 - val_accuracy: 0.6835\n",
      "Epoch 941/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8109 - accuracy: 0.6787 - val_loss: 1.2370 - val_accuracy: 0.6403\n",
      "Epoch 942/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.6832 - accuracy: 0.7650 - val_loss: 1.1664 - val_accuracy: 0.6403\n",
      "Epoch 943/1000\n",
      "18/18 [==============================] - 114s 6s/step - loss: 0.7961 - accuracy: 0.6806 - val_loss: 1.2897 - val_accuracy: 0.6691\n",
      "Epoch 944/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8527 - accuracy: 0.7028 - val_loss: 1.2200 - val_accuracy: 0.6187\n",
      "Epoch 945/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.8526 - accuracy: 0.6335 - val_loss: 1.3258 - val_accuracy: 0.6906\n",
      "Epoch 946/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.9220 - accuracy: 0.6537 - val_loss: 1.1859 - val_accuracy: 0.6691\n",
      "Epoch 947/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.7737 - accuracy: 0.6973 - val_loss: 1.1512 - val_accuracy: 0.6331\n",
      "Epoch 948/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8863 - accuracy: 0.6027 - val_loss: 1.2330 - val_accuracy: 0.6259\n",
      "Epoch 949/1000\n",
      "18/18 [==============================] - 117s 7s/step - loss: 0.7735 - accuracy: 0.6976 - val_loss: 1.1600 - val_accuracy: 0.6115\n",
      "Epoch 950/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.7485 - accuracy: 0.7384 - val_loss: 1.1903 - val_accuracy: 0.6763\n",
      "Epoch 951/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7704 - accuracy: 0.7403 - val_loss: 1.1894 - val_accuracy: 0.6619\n",
      "Epoch 952/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8153 - accuracy: 0.6993 - val_loss: 1.0809 - val_accuracy: 0.6978\n",
      "Epoch 953/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8335 - accuracy: 0.6962 - val_loss: 1.1021 - val_accuracy: 0.6906\n",
      "Epoch 954/1000\n",
      "18/18 [==============================] - 106s 6s/step - loss: 0.7061 - accuracy: 0.7037 - val_loss: 1.1799 - val_accuracy: 0.6475\n",
      "Epoch 955/1000\n",
      "18/18 [==============================] - 105s 6s/step - loss: 0.7836 - accuracy: 0.7022 - val_loss: 1.2219 - val_accuracy: 0.6691\n",
      "Epoch 956/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8100 - accuracy: 0.6989 - val_loss: 1.1333 - val_accuracy: 0.6763\n",
      "Epoch 957/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.6853 - accuracy: 0.7523 - val_loss: 1.0861 - val_accuracy: 0.6835\n",
      "Epoch 958/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8819 - accuracy: 0.6851 - val_loss: 1.1358 - val_accuracy: 0.6978\n",
      "Epoch 959/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.8718 - accuracy: 0.6630 - val_loss: 1.1286 - val_accuracy: 0.6619\n",
      "Epoch 960/1000\n",
      "18/18 [==============================] - 113s 6s/step - loss: 0.8234 - accuracy: 0.6892 - val_loss: 1.1651 - val_accuracy: 0.6691\n",
      "Epoch 961/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8692 - accuracy: 0.6609 - val_loss: 1.1152 - val_accuracy: 0.6835\n",
      "Epoch 962/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.6881 - accuracy: 0.6717 - val_loss: 1.1823 - val_accuracy: 0.6763\n",
      "Epoch 963/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8136 - accuracy: 0.7213 - val_loss: 1.0436 - val_accuracy: 0.6763\n",
      "Epoch 964/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.8626 - accuracy: 0.6718 - val_loss: 1.1883 - val_accuracy: 0.6691\n",
      "Epoch 965/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.6819 - accuracy: 0.7644 - val_loss: 1.1077 - val_accuracy: 0.6906\n",
      "Epoch 966/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8051 - accuracy: 0.6746 - val_loss: 1.1246 - val_accuracy: 0.7194\n",
      "Epoch 967/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.7847 - accuracy: 0.7376 - val_loss: 1.1828 - val_accuracy: 0.6835\n",
      "Epoch 968/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8625 - accuracy: 0.6658 - val_loss: 1.1742 - val_accuracy: 0.6619\n",
      "Epoch 969/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7936 - accuracy: 0.7297 - val_loss: 1.2011 - val_accuracy: 0.6619\n",
      "Epoch 970/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9536 - accuracy: 0.6141 - val_loss: 1.2895 - val_accuracy: 0.6691\n",
      "Epoch 971/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8391 - accuracy: 0.6341 - val_loss: 1.1547 - val_accuracy: 0.6835\n",
      "Epoch 972/1000\n",
      "18/18 [==============================] - 107s 6s/step - loss: 0.8165 - accuracy: 0.6624 - val_loss: 1.1023 - val_accuracy: 0.6763\n",
      "Epoch 973/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8074 - accuracy: 0.6945 - val_loss: 1.1178 - val_accuracy: 0.6691\n",
      "Epoch 974/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7384 - accuracy: 0.7371 - val_loss: 1.1141 - val_accuracy: 0.6547\n",
      "Epoch 975/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.7521 - accuracy: 0.6769 - val_loss: 1.1153 - val_accuracy: 0.6619\n",
      "Epoch 976/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8203 - accuracy: 0.6886 - val_loss: 1.1056 - val_accuracy: 0.6835\n",
      "Epoch 977/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7841 - accuracy: 0.7242 - val_loss: 1.0898 - val_accuracy: 0.6763\n",
      "Epoch 978/1000\n",
      "18/18 [==============================] - 110s 6s/step - loss: 0.6282 - accuracy: 0.7411 - val_loss: 1.1403 - val_accuracy: 0.7122\n",
      "Epoch 979/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7958 - accuracy: 0.6478 - val_loss: 1.1059 - val_accuracy: 0.6835\n",
      "Epoch 980/1000\n",
      "18/18 [==============================] - 102s 6s/step - loss: 0.5992 - accuracy: 0.7548 - val_loss: 1.1996 - val_accuracy: 0.7050\n",
      "Epoch 981/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.9263 - accuracy: 0.6754 - val_loss: 1.1119 - val_accuracy: 0.6906\n",
      "Epoch 982/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8386 - accuracy: 0.7082 - val_loss: 1.1039 - val_accuracy: 0.7050\n",
      "Epoch 983/1000\n",
      "18/18 [==============================] - 104s 6s/step - loss: 0.8161 - accuracy: 0.7129 - val_loss: 1.1367 - val_accuracy: 0.6619\n",
      "Epoch 984/1000\n",
      "18/18 [==============================] - 111s 6s/step - loss: 0.7132 - accuracy: 0.7314 - val_loss: 1.1042 - val_accuracy: 0.6978\n",
      "Epoch 985/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.8549 - accuracy: 0.6590 - val_loss: 1.1877 - val_accuracy: 0.6763\n",
      "Epoch 986/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7628 - accuracy: 0.7039 - val_loss: 1.2125 - val_accuracy: 0.6403\n",
      "Epoch 987/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7274 - accuracy: 0.7047 - val_loss: 1.1577 - val_accuracy: 0.6619\n",
      "Epoch 988/1000\n",
      "18/18 [==============================] - 101s 6s/step - loss: 0.7440 - accuracy: 0.7171 - val_loss: 1.1548 - val_accuracy: 0.6835\n",
      "Epoch 989/1000\n",
      "18/18 [==============================] - 120s 7s/step - loss: 0.8299 - accuracy: 0.7103 - val_loss: 1.1033 - val_accuracy: 0.7122\n",
      "Epoch 990/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 0.7529 - accuracy: 0.6942 - val_loss: 1.2190 - val_accuracy: 0.6691\n",
      "Epoch 991/1000\n",
      "18/18 [==============================] - 103s 6s/step - loss: 1.0407 - accuracy: 0.7176 - val_loss: 1.1112 - val_accuracy: 0.6978\n",
      "Epoch 992/1000\n",
      "18/18 [==============================] - 153s 9s/step - loss: 0.7104 - accuracy: 0.7443 - val_loss: 1.1137 - val_accuracy: 0.7194\n",
      "Epoch 993/1000\n",
      "18/18 [==============================] - 126s 7s/step - loss: 0.7659 - accuracy: 0.6875 - val_loss: 1.1135 - val_accuracy: 0.6835\n",
      "Epoch 994/1000\n",
      "18/18 [==============================] - 138s 8s/step - loss: 0.8723 - accuracy: 0.6547 - val_loss: 1.1785 - val_accuracy: 0.6619\n",
      "Epoch 995/1000\n",
      "18/18 [==============================] - 136s 8s/step - loss: 0.7020 - accuracy: 0.7491 - val_loss: 1.2106 - val_accuracy: 0.6691\n",
      "Epoch 996/1000\n",
      "18/18 [==============================] - 145s 8s/step - loss: 0.7041 - accuracy: 0.7443 - val_loss: 1.2874 - val_accuracy: 0.6475\n",
      "Epoch 997/1000\n",
      "18/18 [==============================] - 141s 8s/step - loss: 0.8342 - accuracy: 0.7161 - val_loss: 1.1380 - val_accuracy: 0.7050\n",
      "Epoch 998/1000\n",
      "18/18 [==============================] - 134s 7s/step - loss: 0.8547 - accuracy: 0.7011 - val_loss: 1.1473 - val_accuracy: 0.6619\n",
      "Epoch 999/1000\n",
      "18/18 [==============================] - 128s 7s/step - loss: 0.7239 - accuracy: 0.7348 - val_loss: 1.1838 - val_accuracy: 0.6547\n",
      "Epoch 1000/1000\n",
      "18/18 [==============================] - 125s 7s/step - loss: 0.8402 - accuracy: 0.6725 - val_loss: 1.1181 - val_accuracy: 0.6475\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( \n",
    "    train_data_gen, \n",
    "    steps_per_epoch = TRAIN_STEPS, \n",
    "    epochs = EPOCHS*2,\n",
    "#     callbacks = callbacks, \n",
    "    validation_data = valid_data_gen, \n",
    "    validation_steps = VALIDATION_STEPS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network took a very long time to train, and it is obvious that even more epochs can improve the performance, as the tendency is for the accuracy to still increase, and the loss to still decrease respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"cnn_opt_11_11.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 - 12s - loss: 1.1181 - accuracy: 0.6475\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on validation data\n",
    "score = model.evaluate(valid_data_gen, steps = VALIDATION_STEPS, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.118070125579834 / Validation accuracy: 0.6474820375442505\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation loss: {score[0]} / Validation accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 4s - loss: 1.2262 - accuracy: 0.6400\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "score = model.evaluate(test_data_gen, steps = TEST_STEPS, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.2261955738067627 / Test accuracy: 0.6399999856948853\n"
     ]
    }
   ],
   "source": [
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data_gen, steps = TEST_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most likely class\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground-truth classes and class-labels\n",
    "true_classes = test_data_gen.classes\n",
    "class_labels = list(test_data_gen.class_indices.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bike       0.38      0.42      0.40        12\n",
      "         car       0.35      0.55      0.43        11\n",
      "  motorcycle       0.00      0.00      0.00         9\n",
      "       other       0.00      0.00      0.00         6\n",
      "       truck       0.09      0.14      0.11         7\n",
      "         van       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.24        50\n",
      "   macro avg       0.14      0.18      0.16        50\n",
      "weighted avg       0.18      0.24      0.21        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(true_classes, predicted_classes, target_names = class_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 1 2 0 4 0]\n",
      " [4 6 0 0 0 1]\n",
      " [1 3 0 1 3 1]\n",
      " [0 3 1 0 2 0]\n",
      " [1 2 2 0 1 1]\n",
      " [2 2 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(test_data_gen.classes, predicted_classes)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laura/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model.predict(train_data_gen, steps = TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most likely class\n",
    "train_predicted_classes = np.argmax(train_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ground-truth classes and class-labels\n",
    "train_true_classes = train_data_gen.classes\n",
    "train_class_labels = list(train_data_gen.class_indices.keys())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0,\n",
       "       0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0,\n",
       "       0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       4, 0, 0, 0, 0, 0, 1, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], dtype=int32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_true_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
